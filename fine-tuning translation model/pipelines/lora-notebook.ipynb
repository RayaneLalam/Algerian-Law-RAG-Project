{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14337887,"sourceType":"datasetVersion","datasetId":9154260}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:28.544352Z","iopub.execute_input":"2025-12-30T02:39:28.544674Z","iopub.status.idle":"2025-12-30T02:39:28.554007Z","shell.execute_reply.started":"2025-12-30T02:39:28.544646Z","shell.execute_reply":"2025-12-30T02:39:28.553439Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/legal-fine-tuning-data/fine_tuning_data_train.jsonl\n/kaggle/input/legal-fine-tuning-data/fine_tuning_data_val.jsonl\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"#!pip install -q transformers datasets accelerate sentencepiece sacrebleu unbabel-comet evaluate peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:28.555284Z","iopub.execute_input":"2025-12-30T02:39:28.555520Z","iopub.status.idle":"2025-12-30T02:39:28.566485Z","shell.execute_reply.started":"2025-12-30T02:39:28.555500Z","shell.execute_reply":"2025-12-30T02:39:28.565877Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"#pip install --upgrade pyarrow==12.0.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:28.567628Z","iopub.execute_input":"2025-12-30T02:39:28.567883Z","iopub.status.idle":"2025-12-30T02:39:28.580805Z","shell.execute_reply.started":"2025-12-30T02:39:28.567847Z","shell.execute_reply":"2025-12-30T02:39:28.580286Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:28.582026Z","iopub.execute_input":"2025-12-30T02:39:28.582352Z","iopub.status.idle":"2025-12-30T02:39:31.870205Z","shell.execute_reply.started":"2025-12-30T02:39:28.582323Z","shell.execute_reply":"2025-12-30T02:39:31.869268Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.4.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\nRequirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.1)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport numpy as np\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\nfrom tqdm.auto import tqdm\n\n# Set memory optimization environment variables\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    get_linear_schedule_with_warmup,\n    set_seed\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    TaskType,\n    PeftModel\n)\nfrom datasets import Dataset, DatasetDict\nfrom accelerate import Accelerator, DistributedDataParallelKwargs\nfrom torch.utils.data import DataLoader\nimport evaluate\n\n# Set random seed for reproducibility\nset_seed(42)\n\n# Clear CUDA cache\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    # Set memory fraction to prevent over-allocation\n    for i in range(torch.cuda.device_count()):\n        torch.cuda.set_per_process_memory_fraction(0.95, i)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\n\n# Print GPU memory info\nif torch.cuda.is_available():\n    for i in range(torch.cuda.device_count()):\n        props = torch.cuda.get_device_properties(i)\n        print(f\"GPU {i}: {props.name} - {props.total_memory / 1024**3:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:31.871990Z","iopub.execute_input":"2025-12-30T02:39:31.872347Z","iopub.status.idle":"2025-12-30T02:39:32.391971Z","shell.execute_reply.started":"2025-12-30T02:39:31.872288Z","shell.execute_reply":"2025-12-30T02:39:32.391184Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.8.0+cu126\nCUDA available: True\nNumber of GPUs: 2\nGPU 0: Tesla T4 - 14.74 GB\nGPU 1: Tesla T4 - 14.74 GB\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"@dataclass\nclass Config:\n    \"\"\"Training configuration\"\"\"\n    # Model\n    model_name: str = \"facebook/nllb-200-distilled-600M\"\n    \n    # Languages (NLLB codes)\n    src_lang: str = \"fra_Latn\"  # French\n    tgt_lang: str = \"arb_Arab\"  # Arabic\n    \n    # Data\n    train_file: str = \"/kaggle/input/legal-fine-tuning-data/fine_tuning_data_train.jsonl\" \n    val_file: str = \"/kaggle/input/legal-fine-tuning-data/fine_tuning_data_val.jsonl\"  \n    max_length: int = 256  # Can use full length with LoRA\n    \n    # LoRA Configuration\n    lora_r: int = 16  # LoRA rank (higher = more parameters, better quality)\n    lora_alpha: int = 32  # LoRA alpha (scaling factor)\n    lora_dropout: float = 0.05\n    lora_target_modules: List[str] = None  # Will be set automatically\n    \n    # Training\n    num_epochs: int = 5\n    per_device_train_batch_size: int = 8  # Can use larger batch with LoRA!\n    per_device_eval_batch_size: int = 16\n    gradient_accumulation_steps: int = 4\n    learning_rate: float = 3e-4  # Higher LR is better for LoRA\n    weight_decay: float = 0.01\n    warmup_ratio: float = 0.1\n    max_grad_norm: float = 1.0\n    \n    # Optimization\n    fp16: bool = True\n    \n    # Checkpointing (based on optimizer steps)\n    output_dir: str = \"/kaggle/working/nllb-fr-ar-legal-lora\"\n    save_steps: int = 5\n    eval_steps: int = 5\n    logging_steps: int = 2\n    \n    # Evaluation\n    num_beams: int = 5  # Can use more beams with LoRA\n    early_stopping: bool = True\n    \n    # Memory optimization\n    gradient_checkpointing: bool = False  # Not needed with LoRA\n\nconfig = Config()\nos.makedirs(config.output_dir, exist_ok=True)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"LoRA Configuration:\")\nprint(f\"  Rank (r): {config.lora_r}\")\nprint(f\"  Alpha: {config.lora_alpha}\")\nprint(f\"  Dropout: {config.lora_dropout}\")\nprint(f\"  Learning Rate: {config.learning_rate}\")\nprint(\"=\"*80 + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:32.393026Z","iopub.execute_input":"2025-12-30T02:39:32.393409Z","iopub.status.idle":"2025-12-30T02:39:32.403087Z","shell.execute_reply.started":"2025-12-30T02:39:32.393386Z","shell.execute_reply":"2025-12-30T02:39:32.402551Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nLoRA Configuration:\n  Rank (r): 16\n  Alpha: 32\n  Dropout: 0.05\n  Learning Rate: 0.0003\n================================================================================\n\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"def load_jsonl_dataset(file_path: str) -> List[Dict[str, str]]:\n    \"\"\"Load JSONL dataset\"\"\"\n    data = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            data.append(json.loads(line.strip()))\n    return data\n\ndef create_datasets(config: Config):\n    \"\"\"Create train and validation datasets\"\"\"\n    \n    # Load raw data\n    print(\"Loading datasets...\")\n    train_data = load_jsonl_dataset(config.train_file)\n    val_data = load_jsonl_dataset(config.val_file)\n    \n    print(f\"Train samples: {len(train_data)}\")\n    print(f\"Validation samples: {len(val_data)}\")\n    \n    # Create HuggingFace datasets\n    train_dataset = Dataset.from_list(train_data)\n    val_dataset = Dataset.from_list(val_data)\n    \n    return DatasetDict({\n        \"train\": train_dataset,\n        \"validation\": val_dataset\n    })\n\n# Load datasets\ntry:\n    raw_datasets = create_datasets(config)\n    print(\"\\nExample training sample:\")\n    print(f\"Source: {raw_datasets['train'][0]['src_text'][:100]}...\")\n    print(f\"Target: {raw_datasets['train'][0]['tgt_text'][:100]}...\")\nexcept FileNotFoundError:\n    print(\"⚠️  Dataset files not found. Creating dummy dataset for demonstration...\")\n    # Create dummy dataset for testing\n    dummy_data = [\n        {\"src_text\": f\"Article {i} du code civil français.\", \n         \"tgt_text\": f\"المادة {i} من القانون المدني الفرنسي.\"}\n        for i in range(1, 101)\n    ]\n    raw_datasets = DatasetDict({\n        \"train\": Dataset.from_list(dummy_data[:80]),\n        \"validation\": Dataset.from_list(dummy_data[80:])\n    })\n    print(\"✓ Dummy dataset created for demonstration\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:32.404770Z","iopub.execute_input":"2025-12-30T02:39:32.404988Z","iopub.status.idle":"2025-12-30T02:39:32.439311Z","shell.execute_reply.started":"2025-12-30T02:39:32.404967Z","shell.execute_reply":"2025-12-30T02:39:32.438735Z"}},"outputs":[{"name":"stdout","text":"Loading datasets...\nTrain samples: 300\nValidation samples: 31\n\nExample training sample:\nSource: Il n’y a pas d’infraction, ni de peine ou de mesures de sûreté sans loi....\nTarget: لا جريمة ولا عقوبة أو تدابير أمن بغير قانون.\n\n...\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"print(\"Loading tokenizer and model...\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    config.model_name,\n    src_lang=config.src_lang,\n    tgt_lang=config.tgt_lang\n)\n\n# Get forced_bos_token_id for Arabic\nif hasattr(tokenizer, 'lang_code_to_id'):\n    forced_bos_token_id = tokenizer.lang_code_to_id[config.tgt_lang]\nelse:\n    forced_bos_token_id = tokenizer.convert_tokens_to_ids(config.tgt_lang)\n\nprint(f\"Forced BOS token ID for {config.tgt_lang}: {forced_bos_token_id}\")\n\n# Load base model in fp16 for memory efficiency\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(\n    config.model_name,\n    torch_dtype=torch.float16 if config.fp16 else torch.float32\n)\nbase_model.config.forced_bos_token_id = forced_bos_token_id\n\nprint(f\"Base model parameters: {base_model.num_parameters():,}\")\nprint(f\"Base model dtype: {base_model.dtype}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:32.440084Z","iopub.execute_input":"2025-12-30T02:39:32.440416Z","iopub.status.idle":"2025-12-30T02:39:38.930638Z","shell.execute_reply.started":"2025-12-30T02:39:32.440394Z","shell.execute_reply":"2025-12-30T02:39:38.929974Z"}},"outputs":[{"name":"stdout","text":"Loading tokenizer and model...\nForced BOS token ID for arb_Arab: 256011\nBase model parameters: 615,073,792\nBase model dtype: torch.float16\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# Configure LoRA\n# Target modules: for NLLB, we target q_proj, v_proj in attention layers\nlora_config = LoraConfig(\n    r=config.lora_r,\n    lora_alpha=config.lora_alpha,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"],  # Transformer layers\n    lora_dropout=config.lora_dropout,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(base_model, lora_config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()\n\nprint(f\"\\n✓ LoRA applied successfully!\")\nprint(f\"Total parameters: {model.num_parameters():,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\nprint(f\"Trainable %: {100 * sum(p.numel() for p in model.parameters() if p.requires_grad) / model.num_parameters():.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:38.931497Z","iopub.execute_input":"2025-12-30T02:39:38.931706Z","iopub.status.idle":"2025-12-30T02:39:39.236932Z","shell.execute_reply.started":"2025-12-30T02:39:38.931685Z","shell.execute_reply":"2025-12-30T02:39:39.236338Z"}},"outputs":[{"name":"stdout","text":"trainable params: 8,650,752 || all params: 623,724,544 || trainable%: 1.3870\n\n✓ LoRA applied successfully!\nTotal parameters: 623,724,544\nTrainable parameters: 8,650,752\nTrainable %: 1.39%\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"def preprocess_function(examples):\n    \"\"\"Tokenize inputs and targets\"\"\"\n    # Set source language\n    tokenizer.src_lang = config.src_lang\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        examples[\"src_text\"],\n        max_length=config.max_length,\n        truncation=True,\n        padding=False\n    )\n    \n    # Tokenize targets\n    tokenizer.src_lang = config.tgt_lang\n    labels = tokenizer(\n        examples[\"tgt_text\"],\n        max_length=config.max_length,\n        truncation=True,\n        padding=False\n    )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply tokenization\nprint(\"Tokenizing datasets...\")\ntokenized_datasets = raw_datasets.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=raw_datasets[\"train\"].column_names,\n    desc=\"Tokenizing\"\n)\n\nprint(f\"Tokenized train samples: {len(tokenized_datasets['train'])}\")\nprint(f\"Tokenized val samples: {len(tokenized_datasets['validation'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:39.237788Z","iopub.execute_input":"2025-12-30T02:39:39.238036Z","iopub.status.idle":"2025-12-30T02:39:40.113471Z","shell.execute_reply.started":"2025-12-30T02:39:39.238014Z","shell.execute_reply":"2025-12-30T02:39:40.112768Z"}},"outputs":[{"name":"stdout","text":"Tokenizing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf0883875bed483cb3092d2cb10d8fe9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/31 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b54c02bd40e34b1b95813a6bb09793c1"}},"metadata":{}},{"name":"stdout","text":"Tokenized train samples: 300\nTokenized val samples: 31\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"@dataclass\nclass DataCollatorForSeq2Seq:\n    \"\"\"Custom data collator with dynamic padding\"\"\"\n    tokenizer: AutoTokenizer\n    model: Optional[AutoModelForSeq2SeqLM] = None\n    padding: bool = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    label_pad_token_id: int = -100\n    \n    def __call__(self, features: List[Dict[str, List[int]]]) -> Dict[str, torch.Tensor]:\n        labels = [f[\"labels\"] for f in features] if \"labels\" in features[0] else None\n        features_without_labels = [{k: v for k, v in f.items() if k != \"labels\"} for f in features]\n        \n        batch = self.tokenizer.pad(\n            features_without_labels,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\"\n        )\n        \n        if labels is not None:\n            max_label_length = max(len(l) for l in labels)\n            if self.pad_to_multiple_of is not None:\n                max_label_length = (\n                    (max_label_length + self.pad_to_multiple_of - 1)\n                    // self.pad_to_multiple_of\n                    * self.pad_to_multiple_of\n                )\n            \n            padded_labels = []\n            for label in labels:\n                remainder = [self.label_pad_token_id] * (max_label_length - len(label))\n                padded_labels.append(label + remainder)\n            \n            batch[\"labels\"] = torch.tensor(padded_labels)\n        \n        return batch\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    pad_to_multiple_of=8 if config.fp16 else None\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:40.114439Z","iopub.execute_input":"2025-12-30T02:39:40.115104Z","iopub.status.idle":"2025-12-30T02:39:40.122788Z","shell.execute_reply.started":"2025-12-30T02:39:40.115073Z","shell.execute_reply":"2025-12-30T02:39:40.122224Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"# Configure DDP\nddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=False)\n\n# Initialize accelerator\naccelerator = Accelerator(\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    mixed_precision=\"fp16\" if config.fp16 else \"no\",\n    log_with=None,\n    kwargs_handlers=[ddp_kwargs]\n)\n\nprint(f\"Distributed type: {accelerator.distributed_type}\")\nprint(f\"Number of processes: {accelerator.num_processes}\")\nprint(f\"Process index: {accelerator.process_index}\")\nprint(f\"Device: {accelerator.device}\")\n\n# Create dataloaders\ntrain_dataloader = DataLoader(\n    tokenized_datasets[\"train\"],\n    batch_size=config.per_device_train_batch_size,\n    shuffle=True,\n    collate_fn=data_collator,\n    num_workers=2,\n    pin_memory=True\n)\n\neval_dataloader = DataLoader(\n    tokenized_datasets[\"validation\"],\n    batch_size=config.per_device_eval_batch_size,\n    shuffle=False,\n    collate_fn=data_collator,\n    num_workers=2,\n    pin_memory=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:40.123623Z","iopub.execute_input":"2025-12-30T02:39:40.123887Z","iopub.status.idle":"2025-12-30T02:39:40.145797Z","shell.execute_reply.started":"2025-12-30T02:39:40.123862Z","shell.execute_reply":"2025-12-30T02:39:40.145152Z"}},"outputs":[{"name":"stdout","text":"Distributed type: DistributedType.NO\nNumber of processes: 1\nProcess index: 0\nDevice: cuda\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"# Optimizer - only optimize LoRA parameters\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=config.learning_rate,\n    weight_decay=config.weight_decay\n)\n\n# Calculate total training steps\nnum_update_steps_per_epoch = len(train_dataloader) // config.gradient_accumulation_steps\nmax_train_steps = config.num_epochs * num_update_steps_per_epoch\nnum_warmup_steps = int(max_train_steps * config.warmup_ratio)\n\n# Scheduler\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=max_train_steps\n)\n\nprint(f\"Total training steps: {max_train_steps}\")\nprint(f\"Warmup steps: {num_warmup_steps}\")\nprint(f\"Steps per epoch: {num_update_steps_per_epoch}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:40.147894Z","iopub.execute_input":"2025-12-30T02:39:40.148162Z","iopub.status.idle":"2025-12-30T02:39:40.168780Z","shell.execute_reply.started":"2025-12-30T02:39:40.148141Z","shell.execute_reply":"2025-12-30T02:39:40.168040Z"}},"outputs":[{"name":"stdout","text":"Total training steps: 45\nWarmup steps: 4\nSteps per epoch: 9\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"# Prepare everything with accelerator\nmodel, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n)\n\nprint(\"✓ Model and dataloaders prepared for distributed training\")\nprint(f\"✓ Model now on device: {accelerator.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:40.169474Z","iopub.execute_input":"2025-12-30T02:39:40.169721Z","iopub.status.idle":"2025-12-30T02:39:40.574716Z","shell.execute_reply.started":"2025-12-30T02:39:40.169701Z","shell.execute_reply":"2025-12-30T02:39:40.574118Z"}},"outputs":[{"name":"stdout","text":"✓ Model and dataloaders prepared for distributed training\n✓ Model now on device: cuda\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:40.575585Z","iopub.execute_input":"2025-12-30T02:39:40.575833Z","iopub.status.idle":"2025-12-30T02:39:43.761147Z","shell.execute_reply.started":"2025-12-30T02:39:40.575811Z","shell.execute_reply":"2025-12-30T02:39:43.760354Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\nRequirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"# Load metrics\nbleu_metric = evaluate.load(\"sacrebleu\")\n\ndef postprocess_text(preds, labels):\n    \"\"\"Post-process predictions and labels for metric computation\"\"\"\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\ndef compute_metrics(predictions, references):\n    \"\"\"Compute BLEU score\"\"\"\n    decoded_preds, decoded_labels = postprocess_text(predictions, references)\n    result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n    return {\"bleu\": result[\"score\"]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:43.762694Z","iopub.execute_input":"2025-12-30T02:39:43.762990Z","iopub.status.idle":"2025-12-30T02:39:45.004501Z","shell.execute_reply.started":"2025-12-30T02:39:43.762960Z","shell.execute_reply":"2025-12-30T02:39:45.003778Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"def evaluate_model(model, eval_dataloader, accelerator, config, forced_bos_token_id):\n    \"\"\"Evaluate model on validation set\"\"\"\n    model.eval()\n    \n    all_predictions = []\n    all_references = []\n    total_loss = 0\n    \n    # Clear cache before evaluation\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\", disable=not accelerator.is_local_main_process):\n            # Compute loss\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_loss += accelerator.gather(loss).mean().item()\n            \n            # Generate predictions\n            generated_tokens = accelerator.unwrap_model(model).generate(\n    input_ids=batch[\"input_ids\"],\n    attention_mask=batch[\"attention_mask\"],\n    max_length=config.max_length,\n    num_beams=config.num_beams,\n    early_stopping=config.early_stopping,\n    forced_bos_token_id=forced_bos_token_id\n            )\n            \n            # Gather predictions and labels from all processes\n            generated_tokens = accelerator.pad_across_processes(\n                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n            )\n            labels = accelerator.pad_across_processes(\n                batch[\"labels\"], dim=1, pad_index=-100\n            )\n            \n            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n            labels = accelerator.gather(labels).cpu().numpy()\n            \n            # Replace -100 in labels\n            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n            \n            # Decode\n            if isinstance(generated_tokens, tuple):\n                generated_tokens = generated_tokens[0]\n            \n            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n            \n            all_predictions.extend(decoded_preds)\n            all_references.extend(decoded_labels)\n    \n    # Compute metrics only on main process\n    metrics = {}\n    if accelerator.is_main_process:\n        avg_loss = total_loss / len(eval_dataloader)\n        metrics = compute_metrics(all_predictions, all_references)\n        metrics[\"eval_loss\"] = avg_loss\n    \n    model.train()\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:45.005471Z","iopub.execute_input":"2025-12-30T02:39:45.005736Z","iopub.status.idle":"2025-12-30T02:39:45.014140Z","shell.execute_reply.started":"2025-12-30T02:39:45.005714Z","shell.execute_reply":"2025-12-30T02:39:45.013503Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"def train(model, train_dataloader, eval_dataloader, optimizer, lr_scheduler, accelerator, config, forced_bos_token_id):\n    \"\"\"Main training loop\"\"\"\n    \n    global_step = 0\n    best_bleu = 0\n    \n    # Calculate actual optimizer steps per epoch\n    optimizer_steps_per_epoch = len(train_dataloader) // config.gradient_accumulation_steps\n    \n    accelerator.print(\"=\" * 80)\n    accelerator.print(\"Starting LoRA fine-tuning...\")\n    accelerator.print(f\"Dataloader steps per epoch: {len(train_dataloader)}\")\n    accelerator.print(f\"Gradient accumulation steps: {config.gradient_accumulation_steps}\")\n    accelerator.print(f\"Optimizer steps per epoch: {optimizer_steps_per_epoch}\")\n    accelerator.print(f\"Total optimizer steps: {max_train_steps}\")\n    accelerator.print(f\"Evaluation every {config.eval_steps} optimizer steps\")\n    accelerator.print(f\"Checkpoint every {config.save_steps} optimizer steps\")\n    accelerator.print(\"=\" * 80)\n    \n    model.train()\n    \n    for epoch in range(config.num_epochs):\n        accelerator.print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n        \n        # Clear cache at start of each epoch\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        progress_bar = tqdm(\n            total=len(train_dataloader),\n            disable=not accelerator.is_local_main_process,\n            desc=f\"Epoch {epoch + 1}\"\n        )\n        \n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                \n                accelerator.backward(loss)\n                \n                if accelerator.sync_gradients:\n                    accelerator.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n                \n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n            \n            if accelerator.sync_gradients:\n                global_step += 1\n                progress_bar.update(1)\n                \n                # Logging\n                if global_step % config.logging_steps == 0:\n                    avg_loss = accelerator.gather(loss).mean().item()\n                    accelerator.print(\n                        f\"Step {global_step} | Loss: {avg_loss:.4f} | LR: {lr_scheduler.get_last_lr()[0]:.2e}\"\n                    )\n                \n                # Evaluation\n                if global_step % config.eval_steps == 0:\n                    accelerator.print(\"\\n\" + \"=\" * 80)\n                    accelerator.print(f\"Evaluating at step {global_step}...\")\n                    \n                    metrics = evaluate_model(model, eval_dataloader, accelerator, config, forced_bos_token_id)\n                    \n                    if accelerator.is_main_process:\n                        accelerator.print(f\"Eval Loss: {metrics['eval_loss']:.4f}\")\n                        accelerator.print(f\"BLEU Score: {metrics['bleu']:.2f}\")\n                        \n                        # Save best model\n                        if metrics['bleu'] > best_bleu:\n                            best_bleu = metrics['bleu']\n                            accelerator.print(f\"✓ New best BLEU: {best_bleu:.2f}\")\n                            \n                            # Save LoRA adapter\n                            unwrapped_model = accelerator.unwrap_model(model)\n                            unwrapped_model.save_pretrained(\n                                config.output_dir,\n                                is_main_process=accelerator.is_main_process,\n                                save_function=accelerator.save\n                            )\n                            tokenizer.save_pretrained(config.output_dir)\n                            accelerator.print(f\"✓ LoRA adapter saved to {config.output_dir}\")\n                    \n                    accelerator.print(\"=\" * 80 + \"\\n\")\n                \n                # Save periodic checkpoint\n                if global_step % config.save_steps == 0:\n                    checkpoint_dir = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n                    os.makedirs(checkpoint_dir, exist_ok=True)\n                    \n                    unwrapped_model = accelerator.unwrap_model(model)\n                    unwrapped_model.save_pretrained(\n                        checkpoint_dir,\n                        is_main_process=accelerator.is_main_process,\n                        save_function=accelerator.save\n                    )\n                    if accelerator.is_main_process:\n                        tokenizer.save_pretrained(checkpoint_dir)\n                        accelerator.print(f\"✓ Checkpoint saved at step {global_step}\")\n        \n        progress_bar.close()\n    \n    accelerator.print(\"\\n\" + \"=\" * 80)\n    accelerator.print(\"Training completed!\")\n    accelerator.print(f\"Best BLEU score: {best_bleu:.2f}\")\n    accelerator.print(\"=\" * 80)\n    \n    return best_bleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:45.015231Z","iopub.execute_input":"2025-12-30T02:39:45.015539Z","iopub.status.idle":"2025-12-30T02:39:45.037367Z","shell.execute_reply.started":"2025-12-30T02:39:45.015515Z","shell.execute_reply":"2025-12-30T02:39:45.036585Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"best_bleu = train(\n    model=model,\n    train_dataloader=train_dataloader,\n    eval_dataloader=eval_dataloader,\n    optimizer=optimizer,\n    lr_scheduler=lr_scheduler,\n    accelerator=accelerator,\n    config=config,\n    forced_bos_token_id=forced_bos_token_id\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:39:45.038258Z","iopub.execute_input":"2025-12-30T02:39:45.038834Z","iopub.status.idle":"2025-12-30T02:40:13.495514Z","shell.execute_reply.started":"2025-12-30T02:39:45.038799Z","shell.execute_reply":"2025-12-30T02:40:13.493732Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nStarting LoRA fine-tuning...\nDataloader steps per epoch: 38\nGradient accumulation steps: 4\nOptimizer steps per epoch: 9\nTotal optimizer steps: 45\nEvaluation every 5 optimizer steps\nCheckpoint every 5 optimizer steps\n================================================================================\n\nEpoch 1/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/38 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c76a1ab3075496d8a163100fa02cd32"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nYou're using a NllbTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a NllbTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Step 2 | Loss: 3.1471 | LR: 1.50e-04\nStep 4 | Loss: 2.2406 | LR: 3.00e-04\n\n================================================================================\nEvaluating at step 5...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32bca3929d9242f2a36d22f7f1b21936"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nYou're using a NllbTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a NllbTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1733: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3648095509.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m best_bleu = train(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0meval_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/4257505667.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, eval_dataloader, optimizer, lr_scheduler, accelerator, config, forced_bos_token_id)\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Evaluating at step {global_step}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforced_bos_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_main_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2656687275.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, eval_dataloader, accelerator, config, forced_bos_token_id)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluating\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_local_main_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, decoder_input_ids, decoder_attention_mask, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   2179\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2180\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2181\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   2182\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/m2m_100/modeling_m2m_100.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             \u001b[0mmasked_lm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1311\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3460\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3461\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3462\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3463\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3464\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.67 GiB. GPU 1 has a total capacity of 14.74 GiB of which 2.05 GiB is free. Process 3951 has 12.69 GiB memory in use. 14.00 GiB allowed; Of the allocated memory 12.15 GiB is allocated by PyTorch, and 419.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 3.67 GiB. GPU 1 has a total capacity of 14.74 GiB of which 2.05 GiB is free. Process 3951 has 12.69 GiB memory in use. 14.00 GiB allowed; Of the allocated memory 12.15 GiB is allocated by PyTorch, and 419.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":74},{"cell_type":"code","source":"if accelerator.is_main_process:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Running final evaluation with COMET...\")\n    print(\"=\" * 80)\n    \n    # Load COMET metric\n    try:\n        comet_metric = evaluate.load(\"comet\")\n        \n        # Prepare data for COMET\n        eval_samples = raw_datasets[\"validation\"].select(range(min(100, len(raw_datasets[\"validation\"]))))\n        sources = eval_samples[\"src_text\"]\n        references = eval_samples[\"tgt_text\"]\n        \n        # Generate translations\n        model.eval()\n        predictions = []\n        \n        for i in tqdm(range(0, len(sources), config.per_device_eval_batch_size), desc=\"Generating\"):\n            batch_sources = sources[i:i + config.per_device_eval_batch_size]\n            \n            tokenizer.src_lang = config.src_lang\n            inputs = tokenizer(\n                batch_sources,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=config.max_length\n            ).to(accelerator.device)\n            \n            with torch.no_grad():\n                generated = accelerator.unwrap_model(model).generate(\n                    **inputs,\n                    max_length=config.max_length,\n                    num_beams=config.num_beams,\n                    early_stopping=True\n                )\n            \n            decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n            predictions.extend(decoded)\n        \n        # Compute COMET\n        comet_input = {\n            \"sources\": sources,\n            \"predictions\": predictions,\n            \"references\": references\n        }\n        \n        comet_score = comet_metric.compute(**comet_input, model_name=\"Unbabel/wmt22-comet-da\")\n        \n        print(f\"\\n✓ Final COMET Score: {comet_score['mean_score']:.4f}\")\n        print(f\"✓ Final BLEU Score: {best_bleu:.2f}\")\n        \n    except Exception as e:\n        print(f\"⚠️  Could not compute COMET score: {e}\")\n        print(\"This is normal on Kaggle due to resource constraints.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:40:13.496850Z","iopub.status.idle":"2025-12-30T02:40:13.497370Z","shell.execute_reply.started":"2025-12-30T02:40:13.497143Z","shell.execute_reply":"2025-12-30T02:40:13.497172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if accelerator.is_main_process:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Saving final LoRA adapter...\")\n    \n    # Save the LoRA adapter (only a few MB!)\n    unwrapped_model = accelerator.unwrap_model(model)\n    unwrapped_model.save_pretrained(config.output_dir)\n    tokenizer.save_pretrained(config.output_dir)\n    \n    # Save config\n    with open(os.path.join(config.output_dir, \"training_config.json\"), \"w\") as f:\n        json.dump(vars(config), f, indent=2)\n    \n    print(f\"✓ LoRA adapter saved to: {config.output_dir}\")\n    print(f\"✓ Adapter size: Only trainable parameters saved (~10-50 MB)\")\n    print(\"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:40:13.498465Z","iopub.status.idle":"2025-12-30T02:40:13.498784Z","shell.execute_reply.started":"2025-12-30T02:40:13.498624Z","shell.execute_reply":"2025-12-30T02:40:13.498645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if accelerator.is_main_process:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Testing inference with LoRA fine-tuned model...\")\n    print(\"=\" * 80)\n    \n    # Reload base model and LoRA adapter for inference\n    print(\"Loading base model...\")\n    inference_base_model = AutoModelForSeq2SeqLM.from_pretrained(\n        config.model_name,\n        torch_dtype=torch.float16 if config.fp16 else torch.float32\n    )\n    \n    print(\"Loading LoRA adapter...\")\n    inference_model = PeftModel.from_pretrained(inference_base_model, config.output_dir)\n    inference_model = inference_model.merge_and_unload()  # Merge LoRA weights into base model\n    \n    inference_tokenizer = AutoTokenizer.from_pretrained(config.output_dir)\n    inference_model.to(accelerator.device)\n    inference_model.eval()\n    \n    # Test samples\n    test_texts = [\n        \"Article 1er : La loi régit l'ensemble du territoire national.\",\n        \"Le code civil régit les droits patrimoniaux et extrapatrimoniaux.\",\n        \"Conformément aux dispositions du journal officiel, le décret entre en vigueur immédiatement.\"\n    ]\n    \n    print(\"\\n🔍 Translation Examples:\\n\")\n    \n    for i, text in enumerate(test_texts, 1):\n        # Tokenize\n        inference_tokenizer.src_lang = config.src_lang\n        inputs = inference_tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=config.max_length\n        ).to(accelerator.device)\n        \n        # Generate\n        with torch.no_grad():\n            generated = inference_model.generate(\n                **inputs,\n                max_length=config.max_length,\n                num_beams=5,\n                early_stopping=True\n            )\n        \n        # Decode\n        translation = inference_tokenizer.decode(generated[0], skip_special_tokens=True)\n        \n        print(f\"Example {i}:\")\n        print(f\"  FR: {text}\")\n        print(f\"  AR: {translation}\")\n        print()\n    \n    print(\"=\" * 80)\n    print(\"✅ LoRA fine-tuning and evaluation complete!\")\n    print(f\"📁 LoRA adapter location: {config.output_dir}\")\n    print(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:40:13.500165Z","iopub.status.idle":"2025-12-30T02:40:13.500518Z","shell.execute_reply.started":"2025-12-30T02:40:13.500325Z","shell.execute_reply":"2025-12-30T02:40:13.500348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n# ========================================================================\n# USING THE LoRA FINE-TUNED MODEL\n# ========================================================================\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom peft import PeftModel\n\n# Method 1: Load base model + LoRA adapter separately (saves memory)\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(\n    \"facebook/nllb-200-distilled-600M\",\n    torch_dtype=torch.float16\n)\nmodel = PeftModel.from_pretrained(base_model, \"/kaggle/working/nllb-fr-ar-legal-lora\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/nllb-fr-ar-legal-lora\")\n\n# Method 2: Merge LoRA weights into base model (for deployment)\nbase_model = AutoModelFor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:40:13.502896Z","iopub.status.idle":"2025-12-30T02:40:13.503199Z","shell.execute_reply.started":"2025-12-30T02:40:13.503076Z","shell.execute_reply":"2025-12-30T02:40:13.503091Z"}},"outputs":[],"execution_count":null}]}