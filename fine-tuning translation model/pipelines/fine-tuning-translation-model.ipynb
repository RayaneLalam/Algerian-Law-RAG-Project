{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14337639,"sourceType":"datasetVersion","datasetId":9154107}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:34.620647Z","iopub.execute_input":"2025-12-30T02:01:34.621290Z","iopub.status.idle":"2025-12-30T02:01:34.630253Z","shell.execute_reply.started":"2025-12-30T02:01:34.621258Z","shell.execute_reply":"2025-12-30T02:01:34.629564Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/fine-tuning-jsonl-data/fine_tuning_data_train.jsonl\n/kaggle/input/fine-tuning-jsonl-data/fine_tuning_data_val.jsonl\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"#!pip install -q transformers datasets accelerate sentencepiece sacrebleu unbabel-comet evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:34.631592Z","iopub.execute_input":"2025-12-30T02:01:34.631920Z","iopub.status.idle":"2025-12-30T02:01:34.643783Z","shell.execute_reply.started":"2025-12-30T02:01:34.631896Z","shell.execute_reply":"2025-12-30T02:01:34.642974Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"#pip install --upgrade pyarrow==12.0.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:34.670873Z","iopub.execute_input":"2025-12-30T02:01:34.671275Z","iopub.status.idle":"2025-12-30T02:01:34.674310Z","shell.execute_reply.started":"2025-12-30T02:01:34.671253Z","shell.execute_reply":"2025-12-30T02:01:34.673606Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:34.682990Z","iopub.execute_input":"2025-12-30T02:01:34.683480Z","iopub.status.idle":"2025-12-30T02:01:38.297760Z","shell.execute_reply.started":"2025-12-30T02:01:34.683455Z","shell.execute_reply":"2025-12-30T02:01:38.296556Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.4.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\nRequirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.1)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport numpy as np\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\nfrom tqdm.auto import tqdm\n\n# Set memory optimization environment variables\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    get_linear_schedule_with_warmup,\n    set_seed\n)\nfrom datasets import Dataset, DatasetDict\nfrom accelerate import Accelerator, DistributedDataParallelKwargs\nfrom torch.utils.data import DataLoader\nimport evaluate\n\n# Set random seed for reproducibility\nset_seed(42)\n\n# Clear CUDA cache\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:38.299500Z","iopub.execute_input":"2025-12-30T02:01:38.299772Z","iopub.status.idle":"2025-12-30T02:01:38.504546Z","shell.execute_reply.started":"2025-12-30T02:01:38.299746Z","shell.execute_reply":"2025-12-30T02:01:38.503706Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.8.0+cu126\nCUDA available: True\nNumber of GPUs: 2\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"@dataclass\nclass Config:\n    \"\"\"Training configuration\"\"\"\n    # Model\n    model_name: str = \"facebook/nllb-200-distilled-600M\"\n    \n    # Languages (NLLB codes)\n    src_lang: str = \"fra_Latn\"  # French\n    tgt_lang: str = \"arb_Arab\"  # Arabic\n    \n    # Data\n    train_file: str = \"/kaggle/input/fine-tuning-jsonl-data/fine_tuning_data_train.jsonl\" \n    val_file: str = \"/kaggle/input/fine-tuning-jsonl-data/fine_tuning_data_val.jsonl\"  \n    max_length: int = 256\n    \n    # Training\n    num_epochs: int = 5\n    per_device_train_batch_size: int = 2 \n    per_device_eval_batch_size: int = 4   \n    gradient_accumulation_steps: int = 16  \n    learning_rate: float = 2e-5\n    weight_decay: float = 0.01\n    warmup_ratio: float = 0.1\n    max_grad_norm: float = 1.0\n    \n    # Optimization\n    fp16: bool = True\n    \n    # Checkpointing\n    output_dir: str = \"/kaggle/working/nllb-fr-ar-legal\"\n    save_steps: int = 5\n    eval_steps: int = 5\n    logging_steps: int = 2\n    \n    # Evaluation\n    num_beams: int = 3  # Reduced from 5 to save memory\n    early_stopping: bool = True\n    \n    # Memory optimization\n    gradient_checkpointing: bool = True\n\nconfig = Config()\nos.makedirs(config.output_dir, exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:38.505672Z","iopub.execute_input":"2025-12-30T02:01:38.506097Z","iopub.status.idle":"2025-12-30T02:01:38.514131Z","shell.execute_reply.started":"2025-12-30T02:01:38.506071Z","shell.execute_reply":"2025-12-30T02:01:38.513193Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def load_jsonl_dataset(file_path: str) -> List[Dict[str, str]]:\n    \"\"\"Load JSONL dataset\"\"\"\n    data = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            data.append(json.loads(line.strip()))\n    return data\n\ndef create_datasets(config: Config):\n    \"\"\"Create train and validation datasets\"\"\"\n    \n    # Load raw data\n    print(\"Loading datasets...\")\n    train_data = load_jsonl_dataset(config.train_file)\n    val_data = load_jsonl_dataset(config.val_file)\n    \n    print(f\"Train samples: {len(train_data)}\")\n    print(f\"Validation samples: {len(val_data)}\")\n    \n    # Create HuggingFace datasets\n    train_dataset = Dataset.from_list(train_data)\n    val_dataset = Dataset.from_list(val_data)\n    \n    return DatasetDict({\n        \"train\": train_dataset,\n        \"validation\": val_dataset\n    })\n\n# Load datasets\ntry:\n    raw_datasets = create_datasets(config)\n    print(\"\\nExample training sample:\")\n    print(f\"Source: {raw_datasets['train'][0]['src_text'][:100]}...\")\n    print(f\"Target: {raw_datasets['train'][0]['tgt_text'][:100]}...\")\nexcept FileNotFoundError:\n    print(\"⚠️  Dataset files not found.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:38.516004Z","iopub.execute_input":"2025-12-30T02:01:38.516503Z","iopub.status.idle":"2025-12-30T02:01:38.551726Z","shell.execute_reply.started":"2025-12-30T02:01:38.516477Z","shell.execute_reply":"2025-12-30T02:01:38.551069Z"}},"outputs":[{"name":"stdout","text":"Loading datasets...\nTrain samples: 300\nValidation samples: 31\n\nExample training sample:\nSource: Il n’y a pas d’infraction, ni de peine ou de mesures de sûreté sans loi....\nTarget: لا جريمة ولا عقوبة أو تدابير أمن بغير قانون.\n\n...\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"print(\"Loading tokenizer and model...\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    config.model_name,\n    src_lang=config.src_lang,\n    tgt_lang=config.tgt_lang\n)\n\n# Get forced_bos_token_id for Arabic\n# NLLB tokenizer stores language codes in different ways\nif hasattr(tokenizer, 'lang_code_to_id'):\n    forced_bos_token_id = tokenizer.lang_code_to_id[config.tgt_lang]\nelse:\n    # For NllbTokenizerFast, convert language code to token id\n    forced_bos_token_id = tokenizer.convert_tokens_to_ids(config.tgt_lang)\n\nprint(f\"Forced BOS token ID for {config.tgt_lang}: {forced_bos_token_id}\")\n\n# Load model\nmodel = AutoModelForSeq2SeqLM.from_pretrained(config.model_name)\nmodel.config.forced_bos_token_id = forced_bos_token_id\n\n# Enable gradient checkpointing to save memory\nif config.gradient_checkpointing:\n    model.gradient_checkpointing_enable()\n    print(\"✓ Gradient checkpointing enabled\")\n\nprint(f\"Model parameters: {model.num_parameters():,}\")\nprint(f\"Model loaded on CPU (will be moved to GPU by Accelerator)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:38.552576Z","iopub.execute_input":"2025-12-30T02:01:38.552868Z","iopub.status.idle":"2025-12-30T02:01:44.078089Z","shell.execute_reply.started":"2025-12-30T02:01:38.552831Z","shell.execute_reply":"2025-12-30T02:01:44.077384Z"}},"outputs":[{"name":"stdout","text":"Loading tokenizer and model...\nForced BOS token ID for arb_Arab: 256011\n✓ Gradient checkpointing enabled\nModel parameters: 615,073,792\nModel loaded on CPU (will be moved to GPU by Accelerator)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"def preprocess_function(examples):\n    \"\"\"Tokenize inputs and targets\"\"\"\n    # Set source language\n    tokenizer.src_lang = config.src_lang\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        examples[\"src_text\"],\n        max_length=config.max_length,\n        truncation=True,\n        padding=False  # We'll pad dynamically\n    )\n    \n    # Tokenize targets\n    tokenizer.src_lang = config.tgt_lang  # Switch to target for proper encoding\n    labels = tokenizer(\n        examples[\"tgt_text\"],\n        max_length=config.max_length,\n        truncation=True,\n        padding=False\n    )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply tokenization\nprint(\"Tokenizing datasets...\")\ntokenized_datasets = raw_datasets.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=raw_datasets[\"train\"].column_names,\n    desc=\"Tokenizing\"\n)\n\nprint(f\"Tokenized train samples: {len(tokenized_datasets['train'])}\")\nprint(f\"Tokenized val samples: {len(tokenized_datasets['validation'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:44.079059Z","iopub.execute_input":"2025-12-30T02:01:44.079429Z","iopub.status.idle":"2025-12-30T02:01:44.925347Z","shell.execute_reply.started":"2025-12-30T02:01:44.079404Z","shell.execute_reply":"2025-12-30T02:01:44.924502Z"}},"outputs":[{"name":"stdout","text":"Tokenizing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd2b566bee44c4999954859fcc484cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/31 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a16e44b3e3b4dd78ec914680ffc17cc"}},"metadata":{}},{"name":"stdout","text":"Tokenized train samples: 300\nTokenized val samples: 31\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"@dataclass\nclass DataCollatorForSeq2Seq:\n    \"\"\"Custom data collator with dynamic padding\"\"\"\n    tokenizer: AutoTokenizer\n    model: Optional[AutoModelForSeq2SeqLM] = None\n    padding: bool = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    label_pad_token_id: int = -100\n    \n    def __call__(self, features: List[Dict[str, List[int]]]) -> Dict[str, torch.Tensor]:\n        # Separate labels from inputs\n        labels = [f[\"labels\"] for f in features] if \"labels\" in features[0] else None\n        \n        # Remove labels from features for input padding\n        features_without_labels = [{k: v for k, v in f.items() if k != \"labels\"} for f in features]\n        \n        # Pad inputs\n        batch = self.tokenizer.pad(\n            features_without_labels,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\"\n        )\n        \n        # Pad labels\n        if labels is not None:\n            max_label_length = max(len(l) for l in labels)\n            if self.pad_to_multiple_of is not None:\n                max_label_length = (\n                    (max_label_length + self.pad_to_multiple_of - 1)\n                    // self.pad_to_multiple_of\n                    * self.pad_to_multiple_of\n                )\n            \n            padded_labels = []\n            for label in labels:\n                remainder = [self.label_pad_token_id] * (max_label_length - len(label))\n                padded_labels.append(label + remainder)\n            \n            batch[\"labels\"] = torch.tensor(padded_labels)\n        \n        return batch\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    pad_to_multiple_of=8 if config.fp16 else None\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:44.926432Z","iopub.execute_input":"2025-12-30T02:01:44.926688Z","iopub.status.idle":"2025-12-30T02:01:44.935530Z","shell.execute_reply.started":"2025-12-30T02:01:44.926664Z","shell.execute_reply":"2025-12-30T02:01:44.934777Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Configure DDP\nddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=False)\n\n# Initialize accelerator\naccelerator = Accelerator(\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    mixed_precision=\"fp16\" if config.fp16 else \"no\",\n    log_with=None,\n    kwargs_handlers=[ddp_kwargs]\n)\n\nprint(f\"Distributed type: {accelerator.distributed_type}\")\nprint(f\"Number of processes: {accelerator.num_processes}\")\nprint(f\"Process index: {accelerator.process_index}\")\nprint(f\"Device: {accelerator.device}\")\n\n# Create dataloaders\ntrain_dataloader = DataLoader(\n    tokenized_datasets[\"train\"],\n    batch_size=config.per_device_train_batch_size,\n    shuffle=True,\n    collate_fn=data_collator,\n    num_workers=0,  # Reduced to save memory\n    pin_memory=True\n)\n\neval_dataloader = DataLoader(\n    tokenized_datasets[\"validation\"],\n    batch_size=config.per_device_eval_batch_size,\n    shuffle=False,\n    collate_fn=data_collator,\n    num_workers=0,  # Reduced to save memory\n    pin_memory=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:44.936497Z","iopub.execute_input":"2025-12-30T02:01:44.936804Z","iopub.status.idle":"2025-12-30T02:01:44.957901Z","shell.execute_reply.started":"2025-12-30T02:01:44.936766Z","shell.execute_reply":"2025-12-30T02:01:44.957170Z"}},"outputs":[{"name":"stdout","text":"Distributed type: DistributedType.NO\nNumber of processes: 1\nProcess index: 0\nDevice: cuda\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Optimizer\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=config.learning_rate,\n    weight_decay=config.weight_decay\n)\n\n# Calculate total training steps\nnum_update_steps_per_epoch = len(train_dataloader) // config.gradient_accumulation_steps\nmax_train_steps = config.num_epochs * num_update_steps_per_epoch\nnum_warmup_steps = int(max_train_steps * config.warmup_ratio)\n\n# Scheduler\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=max_train_steps\n)\n\nprint(f\"Total training steps: {max_train_steps}\")\nprint(f\"Warmup steps: {num_warmup_steps}\")\nprint(f\"Steps per epoch: {num_update_steps_per_epoch}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:44.958743Z","iopub.execute_input":"2025-12-30T02:01:44.959036Z","iopub.status.idle":"2025-12-30T02:01:44.977897Z","shell.execute_reply.started":"2025-12-30T02:01:44.959012Z","shell.execute_reply":"2025-12-30T02:01:44.977157Z"}},"outputs":[{"name":"stdout","text":"Total training steps: 45\nWarmup steps: 4\nSteps per epoch: 9\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# Prepare everything with accelerator\nmodel, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n)\n\nprint(\"✓ Model and dataloaders prepared for distributed training\")\nprint(f\"✓ Model now on device: {accelerator.device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:44.980230Z","iopub.execute_input":"2025-12-30T02:01:44.980836Z","iopub.status.idle":"2025-12-30T02:01:45.797360Z","shell.execute_reply.started":"2025-12-30T02:01:44.980812Z","shell.execute_reply":"2025-12-30T02:01:45.796549Z"}},"outputs":[{"name":"stdout","text":"✓ Model and dataloaders prepared for distributed training\n✓ Model now on device: cuda\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:45.798363Z","iopub.execute_input":"2025-12-30T02:01:45.798661Z","iopub.status.idle":"2025-12-30T02:01:49.193952Z","shell.execute_reply.started":"2025-12-30T02:01:45.798636Z","shell.execute_reply":"2025-12-30T02:01:49.193162Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\nRequirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# Load metrics\nbleu_metric = evaluate.load(\"sacrebleu\")\n\ndef postprocess_text(preds, labels):\n    \"\"\"Post-process predictions and labels for metric computation\"\"\"\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]  # BLEU expects list of references\n    return preds, labels\n\ndef compute_metrics(predictions, references):\n    \"\"\"Compute BLEU score\"\"\"\n    decoded_preds, decoded_labels = postprocess_text(predictions, references)\n    result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n    return {\"bleu\": result[\"score\"]}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:49.195572Z","iopub.execute_input":"2025-12-30T02:01:49.195936Z","iopub.status.idle":"2025-12-30T02:01:50.127891Z","shell.execute_reply.started":"2025-12-30T02:01:49.195899Z","shell.execute_reply":"2025-12-30T02:01:50.127206Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"def evaluate_model(model, eval_dataloader, accelerator, config, forced_bos_token_id):\n    \"\"\"Evaluate model on validation set\"\"\"\n    model.eval()\n    \n    all_predictions = []\n    all_references = []\n    total_loss = 0\n    \n    # Clear cache before evaluation\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Disable gradient checkpointing during evaluation\n    if hasattr(model.module if hasattr(model, 'module') else model, 'gradient_checkpointing_disable'):\n        (model.module if hasattr(model, 'module') else model).gradient_checkpointing_disable()\n    \n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\", disable=not accelerator.is_local_main_process):\n            # Compute loss (skip if OOM risk is high)\n            try:\n                outputs = model(**batch)\n                loss = outputs.loss\n                total_loss += accelerator.gather(loss).mean().item()\n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n                    accelerator.print(\"⚠️  Skipping loss computation due to memory\")\n                else:\n                    raise e\n            \n            # Generate predictions with reduced beam search\n            # Don't pass forced_bos_token_id if it causes device issues\n            # The model config already has it set\n            generated_tokens = accelerator.unwrap_model(model).generate(\n                batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                max_length=config.max_length,\n                num_beams=config.num_beams,\n                early_stopping=config.early_stopping,\n                use_cache=True  # Enable KV cache for memory efficiency\n            )\n            \n            # Gather predictions and labels from all processes\n            generated_tokens = accelerator.pad_across_processes(\n                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n            )\n            labels = accelerator.pad_across_processes(\n                batch[\"labels\"], dim=1, pad_index=-100\n            )\n            \n            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n            labels = accelerator.gather(labels).cpu().numpy()\n            \n            # Replace -100 in labels (used for padding)\n            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n            \n            # Decode\n            if isinstance(generated_tokens, tuple):\n                generated_tokens = generated_tokens[0]\n            \n            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n            \n            all_predictions.extend(decoded_preds)\n            all_references.extend(decoded_labels)\n    \n    # Compute metrics only on main process\n    metrics = {}\n    if accelerator.is_main_process:\n        avg_loss = total_loss / len(eval_dataloader) if total_loss > 0 else 0\n        metrics = compute_metrics(all_predictions, all_references)\n        metrics[\"eval_loss\"] = avg_loss\n    \n    # Re-enable gradient checkpointing for training\n    if config.gradient_checkpointing:\n        if hasattr(model.module if hasattr(model, 'module') else model, 'gradient_checkpointing_enable'):\n            (model.module if hasattr(model, 'module') else model).gradient_checkpointing_enable()\n    \n    model.train()\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:50.128791Z","iopub.execute_input":"2025-12-30T02:01:50.129024Z","iopub.status.idle":"2025-12-30T02:01:50.141215Z","shell.execute_reply.started":"2025-12-30T02:01:50.129001Z","shell.execute_reply":"2025-12-30T02:01:50.140379Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def train(model, train_dataloader, eval_dataloader, optimizer, lr_scheduler, accelerator, config, forced_bos_token_id):\n    \"\"\"Main training loop\"\"\"\n    \n    global_step = 0\n    best_bleu = 0\n    \n    accelerator.print(\"=\" * 80)\n    accelerator.print(\"Starting training...\")\n    accelerator.print(\"=\" * 80)\n    \n    model.train()\n    \n    for epoch in range(config.num_epochs):\n        accelerator.print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n        \n        # Clear cache at start of each epoch\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        progress_bar = tqdm(\n            total=len(train_dataloader),\n            disable=not accelerator.is_local_main_process,\n            desc=f\"Epoch {epoch + 1}\"\n        )\n        \n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(model):\n                try:\n                    outputs = model(**batch)\n                    loss = outputs.loss\n                    \n                    accelerator.backward(loss)\n                    \n                    if accelerator.sync_gradients:\n                        accelerator.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n                    \n                    optimizer.step()\n                    lr_scheduler.step()\n                    optimizer.zero_grad()\n                    \n                except RuntimeError as e:\n                    if \"out of memory\" in str(e):\n                        if torch.cuda.is_available():\n                            torch.cuda.empty_cache()\n                        accelerator.print(f\"⚠️  OOM at step {global_step}, skipping batch\")\n                        optimizer.zero_grad()\n                        continue\n                    else:\n                        raise e\n            \n            if accelerator.sync_gradients:\n                global_step += 1\n                progress_bar.update(1)\n                \n                # Logging\n                if global_step % config.logging_steps == 0:\n                    avg_loss = accelerator.gather(loss).mean().item()\n                    accelerator.print(\n                        f\"Step {global_step} | Loss: {avg_loss:.4f} | LR: {lr_scheduler.get_last_lr()[0]:.2e}\"\n                    )\n                \n                # Evaluation\n                if global_step % config.eval_steps == 0:\n                    accelerator.print(\"\\n\" + \"=\" * 80)\n                    accelerator.print(f\"Evaluating at step {global_step}...\")\n                    \n                    metrics = evaluate_model(model, eval_dataloader, accelerator, config, forced_bos_token_id)\n                    \n                    if accelerator.is_main_process:\n                        accelerator.print(f\"Eval Loss: {metrics['eval_loss']:.4f}\")\n                        accelerator.print(f\"BLEU Score: {metrics['bleu']:.2f}\")\n                        \n                        # Save best model\n                        if metrics['bleu'] > best_bleu:\n                            best_bleu = metrics['bleu']\n                            accelerator.print(f\"✓ New best BLEU: {best_bleu:.2f}\")\n                            \n                            # Save checkpoint\n                            unwrapped_model = accelerator.unwrap_model(model)\n                            unwrapped_model.save_pretrained(\n                                config.output_dir,\n                                is_main_process=accelerator.is_main_process,\n                                save_function=accelerator.save\n                            )\n                            tokenizer.save_pretrained(config.output_dir)\n                            accelerator.print(f\"✓ Model saved to {config.output_dir}\")\n                    \n                    accelerator.print(\"=\" * 80 + \"\\n\")\n                \n                # Save periodic checkpoint\n                if global_step % config.save_steps == 0:\n                    checkpoint_dir = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n                    os.makedirs(checkpoint_dir, exist_ok=True)\n                    \n                    unwrapped_model = accelerator.unwrap_model(model)\n                    unwrapped_model.save_pretrained(\n                        checkpoint_dir,\n                        is_main_process=accelerator.is_main_process,\n                        save_function=accelerator.save\n                    )\n                    if accelerator.is_main_process:\n                        tokenizer.save_pretrained(checkpoint_dir)\n                        accelerator.print(f\"✓ Checkpoint saved at step {global_step}\")\n        \n        progress_bar.close()\n    \n    accelerator.print(\"\\n\" + \"=\" * 80)\n    accelerator.print(\"Training completed!\")\n    accelerator.print(f\"Best BLEU score: {best_bleu:.2f}\")\n    accelerator.print(\"=\" * 80)\n    \n    return best_bleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:50.142562Z","iopub.execute_input":"2025-12-30T02:01:50.143308Z","iopub.status.idle":"2025-12-30T02:01:50.162693Z","shell.execute_reply.started":"2025-12-30T02:01:50.143265Z","shell.execute_reply":"2025-12-30T02:01:50.161991Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"best_bleu = train(\n    model=model,\n    train_dataloader=train_dataloader,\n    eval_dataloader=eval_dataloader,\n    optimizer=optimizer,\n    lr_scheduler=lr_scheduler,\n    accelerator=accelerator,\n    config=config,\n    forced_bos_token_id=forced_bos_token_id\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:01:50.163583Z","iopub.execute_input":"2025-12-30T02:01:50.163904Z","iopub.status.idle":"2025-12-30T02:04:01.086211Z","shell.execute_reply.started":"2025-12-30T02:01:50.163868Z","shell.execute_reply":"2025-12-30T02:04:01.085407Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nStarting training...\n================================================================================\n\nEpoch 1/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/150 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99d2d1029b434f779530ecfb86bd405d"}},"metadata":{}},{"name":"stderr","text":"You're using a NllbTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n\nEpoch 2/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/150 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f5ae6ae8c584e57bb5d3c588e979caf"}},"metadata":{}},{"name":"stdout","text":"⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n\nEpoch 3/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/150 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06baa1bc39704f499dedb85edea246b9"}},"metadata":{}},{"name":"stdout","text":"⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n\nEpoch 4/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4:   0%|          | 0/150 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2e4251324954959a0bff53aa19edee2"}},"metadata":{}},{"name":"stdout","text":"⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n\nEpoch 5/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5:   0%|          | 0/150 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8581b9c3d6994fd98830cb8ca6fecaf0"}},"metadata":{}},{"name":"stdout","text":"⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n⚠️  OOM at step 0, skipping batch\n\n================================================================================\nTraining completed!\nBest BLEU score: 0.00\n================================================================================\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"if accelerator.is_main_process:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Running final evaluation with COMET...\")\n    print(\"=\" * 80)\n    \n    # Load COMET metric\n    try:\n        comet_metric = evaluate.load(\"comet\")\n        \n        # Prepare data for COMET\n        eval_samples = raw_datasets[\"validation\"].select(range(min(100, len(raw_datasets[\"validation\"]))))\n        sources = eval_samples[\"src_text\"]\n        references = eval_samples[\"tgt_text\"]\n        \n        # Get forced_bos_token_id for inference\n        if hasattr(tokenizer, 'lang_code_to_id'):\n            inference_forced_bos = tokenizer.lang_code_to_id[config.tgt_lang]\n        else:\n            inference_forced_bos = tokenizer.convert_tokens_to_ids(config.tgt_lang)\n        \n        # Generate translations\n        model.eval()\n        predictions = []\n        \n        for i in tqdm(range(0, len(sources), config.per_device_eval_batch_size), desc=\"Generating\"):\n            batch_sources = sources[i:i + config.per_device_eval_batch_size]\n            \n            tokenizer.src_lang = config.src_lang\n            inputs = tokenizer(\n                batch_sources,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=config.max_length\n            ).to(accelerator.device)\n            \n            with torch.no_grad():\n                # Don't pass forced_bos_token_id, use model config\n                generated = accelerator.unwrap_model(model).generate(\n                    **inputs,\n                    max_length=config.max_length,\n                    num_beams=config.num_beams,\n                    early_stopping=True\n                )\n            \n            decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n            predictions.extend(decoded)\n        \n        # Compute COMET\n        comet_input = {\n            \"sources\": sources,\n            \"predictions\": predictions,\n            \"references\": references\n        }\n        \n        comet_score = comet_metric.compute(**comet_input, model_name=\"Unbabel/wmt22-comet-da\")\n        \n        print(f\"\\n✓ Final COMET Score: {comet_score['mean_score']:.4f}\")\n        print(f\"✓ Final BLEU Score: {best_bleu:.2f}\")\n        \n    except Exception as e:\n        print(f\"⚠️  Could not compute COMET score: {e}\")\n        print(\"This is normal on Kaggle due to resource constraints.\")\n        print(\"COMET requires significant memory and may timeout.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:04:01.619390Z","iopub.execute_input":"2025-12-30T02:04:01.619713Z","iopub.status.idle":"2025-12-30T02:04:02.542052Z","shell.execute_reply.started":"2025-12-30T02:04:01.619687Z","shell.execute_reply":"2025-12-30T02:04:02.541175Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nRunning final evaluation with COMET...\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49f1556034524282843df378fd1eb020"}},"metadata":{}},{"name":"stdout","text":"⚠️  Could not compute COMET score: To be able to use evaluate-metric/comet, you need to install the following dependencies['comet'] using 'pip install unbabel-comet' for instance'\nThis is normal on Kaggle due to resource constraints.\nCOMET requires significant memory and may timeout.\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"if accelerator.is_main_process:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Saving final model...\")\n    \n    # Ensure the best model is saved\n    unwrapped_model = accelerator.unwrap_model(model)\n    unwrapped_model.save_pretrained(config.output_dir)\n    tokenizer.save_pretrained(config.output_dir)\n    \n    # Save config\n    with open(os.path.join(config.output_dir, \"training_config.json\"), \"w\") as f:\n        json.dump(vars(config), f, indent=2)\n    \n    print(f\"✓ Final model saved to: {config.output_dir}\")\n    print(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:04:02.543105Z","iopub.execute_input":"2025-12-30T02:04:02.543472Z","iopub.status.idle":"2025-12-30T02:04:08.020840Z","shell.execute_reply.started":"2025-12-30T02:04:02.543444Z","shell.execute_reply":"2025-12-30T02:04:08.019847Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSaving final model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'forced_bos_token_id': 256011}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✓ Final model saved to: /kaggle/working/nllb-fr-ar-legal\n================================================================================\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"if accelerator.is_main_process:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Testing inference with fine-tuned model...\")\n    print(\"=\" * 80)\n    \n    # Reload model for inference\n    inference_model = AutoModelForSeq2SeqLM.from_pretrained(config.output_dir)\n    inference_tokenizer = AutoTokenizer.from_pretrained(config.output_dir)\n    inference_model.to(accelerator.device)\n    inference_model.eval()\n    \n    # Get forced_bos_token_id\n    if hasattr(inference_tokenizer, 'lang_code_to_id'):\n        tgt_forced_bos = inference_tokenizer.lang_code_to_id[config.tgt_lang]\n    else:\n        tgt_forced_bos = inference_tokenizer.convert_tokens_to_ids(config.tgt_lang)\n    \n    # Test samples\n    test_texts = [\n        \"Article 1er : La loi régit l'ensemble du territoire national.\",\n        \"Le code civil régit les droits patrimoniaux et extrapatrimoniaux.\",\n        \"Conformément aux dispositions du journal officiel, le décret entre en vigueur immédiatement.\"\n    ]\n    \n    print(\"\\n🔍 Translation Examples:\\n\")\n    \n    for i, text in enumerate(test_texts, 1):\n        # Tokenize\n        inference_tokenizer.src_lang = config.src_lang\n        inputs = inference_tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=config.max_length\n        ).to(accelerator.device)\n        \n        # Generate\n        with torch.no_grad():\n            generated = inference_model.generate(\n                **inputs,\n                forced_bos_token_id=tgt_forced_bos,\n                max_length=config.max_length,\n                num_beams=5,\n                early_stopping=True\n            )\n        \n        # Decode\n        translation = inference_tokenizer.decode(generated[0], skip_special_tokens=True)\n        \n        print(f\"Example {i}:\")\n        print(f\"  FR: {text}\")\n        print(f\"  AR: {translation}\")\n        print()\n    \n    print(\"=\" * 80)\n    print(\"✅ Training and evaluation complete!\")\n    print(f\"📁 Model location: {config.output_dir}\")\n    print(\"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:04:08.022222Z","iopub.execute_input":"2025-12-30T02:04:08.022555Z","iopub.status.idle":"2025-12-30T02:04:12.791030Z","shell.execute_reply.started":"2025-12-30T02:04:08.022526Z","shell.execute_reply":"2025-12-30T02:04:12.790056Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nTesting inference with fine-tuned model...\n================================================================================\n\n🔍 Translation Examples:\n\nExample 1:\n  FR: Article 1er : La loi régit l'ensemble du territoire national.\n  AR: المادة 1: القانون ينظم كل الأراضي الوطنية.\n\nExample 2:\n  FR: Le code civil régit les droits patrimoniaux et extrapatrimoniaux.\n  AR: القانون المدني ينظم الحقوق الممتلكة وغير الممتلكة.\n\nExample 3:\n  FR: Conformément aux dispositions du journal officiel, le décret entre en vigueur immédiatement.\n  AR: وفقًا لأحكام الجريدة الرسمية، فإن المرسوم ينطبق على الفور.\n\n================================================================================\n✅ Training and evaluation complete!\n📁 Model location: /kaggle/working/nllb-fr-ar-legal\n================================================================================\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"\n\"\"\"\n# ========================================================================\n# USING THE FINE-TUNED MODEL\n# ========================================================================\n\n# Load the model\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_path = \"/kaggle/working/nllb-fr-ar-legal\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path)\nmodel.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Translation function\ndef translate_french_to_arabic(text, max_length=256):\n    tokenizer.src_lang = \"fra_Latn\"\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    # Get forced_bos_token_id\n    if hasattr(tokenizer, 'lang_code_to_id'):\n        forced_bos = tokenizer.lang_code_to_id[\"arb_Arab\"]\n    else:\n        forced_bos = tokenizer.convert_tokens_to_ids(\"arb_Arab\")\n    \n    generated = model.generate(\n        **inputs,\n        forced_bos_token_id=forced_bos,\n        max_length=max_length,\n        num_beams=5,\n        early_stopping=True\n    )\n    \n    return tokenizer.decode(generated[0], skip_special_tokens=True)\n\n# Example usage\nfrench_text = \"Article 5 : Toute personne a droit à la liberté et à la sécurité.\"\narabic_translation = translate_french_to_arabic(french_text)\nprint(f\"French: {french_text}\")\nprint(f\"Arabic: {arabic_translation}\")\n\"\"\"\n\nprint(\"\\n✅ Notebook execution complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T02:04:12.792408Z","iopub.execute_input":"2025-12-30T02:04:12.792769Z","iopub.status.idle":"2025-12-30T02:04:12.798986Z","shell.execute_reply.started":"2025-12-30T02:04:12.792737Z","shell.execute_reply":"2025-12-30T02:04:12.797826Z"}},"outputs":[{"name":"stdout","text":"\n✅ Notebook execution complete!\n","output_type":"stream"}],"execution_count":50}]}