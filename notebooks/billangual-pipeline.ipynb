{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14365865,"sourceType":"datasetVersion","datasetId":9173583},{"sourceId":14366065,"sourceType":"datasetVersion","datasetId":9173714}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:40:46.877085Z","iopub.execute_input":"2026-01-01T21:40:46.877815Z","iopub.status.idle":"2026-01-01T21:40:47.156211Z","shell.execute_reply.started":"2026-01-01T21:40:46.877784Z","shell.execute_reply":"2026-01-01T21:40:47.155644Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/meta-arabic-data/laws_ar.index.meta\n/kaggle/input/bilangual-data/algerian_legal(joconstitutionpenalecivilcommercefamille) embedder_ dangvantuan-sentence-camembert-large.faiss\n/kaggle/input/bilangual-data/laws_ar.index\n/kaggle/input/bilangual-data/algerian_legal(joconstitutionpenalecivilcommercefamille) embedder_ dangvantuan-sentence-camembert-large_docs.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q transformers torch sentence-transformers faiss-cpu accelerate bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:40:47.157332Z","iopub.execute_input":"2026-01-01T21:40:47.157666Z","iopub.status.idle":"2026-01-01T21:40:56.261975Z","shell.execute_reply.started":"2026-01-01T21:40:47.157642Z","shell.execute_reply":"2026-01-01T21:40:56.261090Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import json\nimport torch\nimport faiss\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom sentence_transformers import SentenceTransformer\nfrom typing import List, Dict\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"‚úÖ All libraries imported successfully!\")\nprint(f\"‚úÖ PyTorch version: {torch.__version__}\")\nprint(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:40:56.263224Z","iopub.execute_input":"2026-01-01T21:40:56.263915Z","iopub.status.idle":"2026-01-01T21:41:41.734406Z","shell.execute_reply.started":"2026-01-01T21:40:56.263885Z","shell.execute_reply":"2026-01-01T21:41:41.733416Z"}},"outputs":[{"name":"stderr","text":"2026-01-01 21:41:18.730532: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767303679.179298      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767303679.298368      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767303680.351252      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767303680.351315      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767303680.351317      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767303680.351320      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ All libraries imported successfully!\n‚úÖ PyTorch version: 2.8.0+cu126\n‚úÖ CUDA available: True\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n# French files\nFRENCH_INDEX_PATH = \"/kaggle/input/bilangual-data/algerian_legal(joconstitutionpenalecivilcommercefamille) embedder_ dangvantuan-sentence-camembert-large.faiss\"\nFRENCH_DOCS_PATH = \"/kaggle/input/bilangual-data/algerian_legal(joconstitutionpenalecivilcommercefamille) embedder_ dangvantuan-sentence-camembert-large_docs.json\"\n\n# Arabic files\nARABIC_INDEX_PATH = \"/kaggle/input/bilangual-data/laws_ar.index\"\n\n# Models\nFRENCH_EMBEDDING_MODEL = \"dangvantuan/sentence-camembert-large\"\nARABIC_EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\nFRENCH_LLM = \"bofenghuang/vigogne-2-7b-chat\"\nARABIC_LLM = \"Qwen/Qwen2.5-7B-Instruct\"\n\nUSE_4BIT_QUANTIZATION = True\nTOP_K_RETRIEVAL = 3\n\nprint(\"‚úÖ Configuration set!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:41:41.736666Z","iopub.execute_input":"2026-01-01T21:41:41.737168Z","iopub.status.idle":"2026-01-01T21:41:41.742927Z","shell.execute_reply.started":"2026-01-01T21:41:41.737144Z","shell.execute_reply":"2026-01-01T21:41:41.742084Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Configuration set!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def detect_script(text: str) -> str:\n    \"\"\"Detect if text is Arabic or French/Latin script\"\"\"\n    arabic_chars = sum(1 for c in text if '\\u0600' <= c <= '\\u06FF')\n    total_chars = sum(1 for c in text if c.isalpha())\n    \n    if total_chars == 0:\n        return \"french\"\n    \n    arabic_ratio = arabic_chars / total_chars\n    return \"arabic\" if arabic_ratio > 0.5 else \"french\"\n\ndef detect_response_language(text: str) -> str:\n    \"\"\"Detect requested response language from query\"\"\"\n    text_lower = text.lower()\n    \n    # Check for explicit language requests\n    arabic_requests = [\"en arabe\", \"in arabic\", \"ÿ®ÿßŸÑÿπÿ±ÿ®Ÿäÿ©\", \"answer in arabic\", \n                      \"r√©pondre en arabe\", \"ÿ£ÿ¨ÿ® ÿ®ÿßŸÑÿπÿ±ÿ®Ÿäÿ©\"]\n    french_requests = [\"en fran√ßais\", \"in french\", \"ÿ®ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿäÿ©\", \"answer in french\", \n                      \"r√©pondre en fran√ßais\", \"ÿ£ÿ¨ÿ® ÿ®ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿäÿ©\"]\n    \n    for req in arabic_requests:\n        if req in text_lower:\n            return \"arabic\"\n    \n    for req in french_requests:\n        if req in text_lower:\n            return \"french\"\n    \n    # Default: same as query language\n    return detect_script(text)\n\nprint(\"‚úÖ Language detection functions defined!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:41:41.743734Z","iopub.execute_input":"2026-01-01T21:41:41.743976Z","iopub.status.idle":"2026-01-01T21:41:41.771990Z","shell.execute_reply.started":"2026-01-01T21:41:41.743949Z","shell.execute_reply":"2026-01-01T21:41:41.771303Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Language detection functions defined!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(\"üîÑ Loading embedding models...\")\n\nfrench_embedder = SentenceTransformer(FRENCH_EMBEDDING_MODEL)\nprint(f\"‚úÖ French embedder loaded: {FRENCH_EMBEDDING_MODEL}\")\n\narabic_embedder = SentenceTransformer(ARABIC_EMBEDDING_MODEL)\nprint(f\"‚úÖ Arabic embedder loaded: {ARABIC_EMBEDDING_MODEL}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:41:41.773049Z","iopub.execute_input":"2026-01-01T21:41:41.773367Z","iopub.status.idle":"2026-01-01T21:41:53.531101Z","shell.execute_reply.started":"2026-01-01T21:41:41.773336Z","shell.execute_reply":"2026-01-01T21:41:53.530471Z"}},"outputs":[{"name":"stdout","text":"üîÑ Loading embedding models...\n","output_type":"stream"},{"name":"stderr","text":"WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name dangvantuan/sentence-camembert-large. Creating a new one with mean pooling.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/683 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"918bd5bc348e48e593c92d1a38f5349f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.35G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7950e84ad33e4754b01718dc7e338c6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/400 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"917da06ec76c4917b63c7b56d2370841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/809k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4042800f2772445980e329c04097d37b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/298 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"507229a96e004bafa2a06ba74d38cc0d"}},"metadata":{}},{"name":"stdout","text":"‚úÖ French embedder loaded: dangvantuan/sentence-camembert-large\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4523229b389430997cea49c5127be6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab478ca4837840408ee4e25c686be3aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"416a2feea5104060858f5252c9a178f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02a7a7db0fb44e43bb7dbe3b6d9fc195"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f2a54d66ee74ddea3f4643e9b8a35d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee37b8226eb411e9826bd09f2a6303e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8084fda66b94aa6bb1d3f1721bbd2f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5af8c6e784b14af28d46b0ab8b8c0c7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d894da28d904634b61b7299eb488546"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1997c7bd5194c9f878b2ff0d21d43c5"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Arabic embedder loaded: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pickle\n\nprint(\"üîÑ Loading FAISS indices and documents...\")\n\n# French\nfrench_index = faiss.read_index(FRENCH_INDEX_PATH)\nwith open(FRENCH_DOCS_PATH, 'r', encoding='utf-8') as f:\n    french_docs = json.load(f)\nprint(f\"‚úÖ French index: {french_index.ntotal} vectors, {len(french_docs)} documents\")\n\n# Arabic\narabic_index = faiss.read_index(ARABIC_INDEX_PATH)\nwith open(\"/kaggle/input/meta-arabic-data/laws_ar.index.meta\", \"rb\") as f:\n    arabic_meta = pickle.load(f)\n\narabic_docs = arabic_meta[\"chunks\"]\nprint(f\"‚úÖ Arabic index: {arabic_index.ntotal} vectors, {len(arabic_docs)} documents\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:41:53.531923Z","iopub.execute_input":"2026-01-01T21:41:53.532186Z","iopub.status.idle":"2026-01-01T21:41:55.413332Z","shell.execute_reply.started":"2026-01-01T21:41:53.532164Z","shell.execute_reply":"2026-01-01T21:41:55.412282Z"}},"outputs":[{"name":"stdout","text":"üîÑ Loading FAISS indices and documents...\n‚úÖ French index: 18722 vectors, 18722 documents\n‚úÖ Arabic index: 12758 vectors, 12758 documents\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(\"üîÑ Loading French LLM...\")\n\nfrench_tokenizer = AutoTokenizer.from_pretrained(FRENCH_LLM)\n\nif USE_4BIT_QUANTIZATION and torch.cuda.is_available():\n    quant_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\"\n    )\n    french_llm = AutoModelForCausalLM.from_pretrained(\n        FRENCH_LLM,\n        quantization_config=quant_config,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\nelse:\n    french_llm = AutoModelForCausalLM.from_pretrained(\n        FRENCH_LLM,\n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n        device_map=\"auto\" if torch.cuda.is_available() else None,\n        trust_remote_code=True\n    )\n\nprint(f\"‚úÖ French LLM loaded: {FRENCH_LLM}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:41:55.414357Z","iopub.execute_input":"2026-01-01T21:41:55.414632Z","iopub.status.idle":"2026-01-01T21:42:50.917210Z","shell.execute_reply.started":"2026-01-01T21:41:55.414601Z","shell.execute_reply":"2026-01-01T21:42:50.916408Z"}},"outputs":[{"name":"stdout","text":"üîÑ Loading French LLM...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11742dde4cfa48e1b0ff304d7063a9e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f23b279ed1f14f6da7c44ee251abfb18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ccfd906be254b889396b8d181b50777"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/670 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c662c65b02e4b9ba907fcd9ab432031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b242c0f4d0a54751a75accf6d9131774"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eceb80b0ba54553a373e724c39582e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00007-of-00007.bin:   0%|          | 0.00/1.66G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"676607158b3e45db8e5f604e42c6bd56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00004-of-00007.bin:   0%|          | 0.00/1.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d091782127104c64ae3b940251bb3dd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00003-of-00007.bin:   0%|          | 0.00/1.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9efe3da1c354a628e5d42d3fa5b7dea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00007.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2743bec7c9e34a7d8b64c0cb4c1299a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00005-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b8f362afdd74820b5999eefde39fb9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00007.bin:   0%|          | 0.00/1.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8bd63540b594b6b8777794b2c68cdf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"907cdd1b631e4f1597734e4b49cb6bb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89dd8318ca404489a198ccd761a57af6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"732d258b53ed4166b776bc293cf07d8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d568ed076ff84b70964ecd3555e2c257"}},"metadata":{}},{"name":"stdout","text":"‚úÖ French LLM loaded: bofenghuang/vigogne-2-7b-chat\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(\"üîÑ Loading Arabic LLM...\")\n\narabic_tokenizer = AutoTokenizer.from_pretrained(ARABIC_LLM)\n\nif USE_4BIT_QUANTIZATION and torch.cuda.is_available():\n    quant_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\"\n    )\n    arabic_llm = AutoModelForCausalLM.from_pretrained(\n        ARABIC_LLM,\n        quantization_config=quant_config,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\nelse:\n    arabic_llm = AutoModelForCausalLM.from_pretrained(\n        ARABIC_LLM,\n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n        device_map=\"auto\" if torch.cuda.is_available() else None,\n        trust_remote_code=True\n    )\n\nprint(f\"‚úÖ Arabic LLM loaded: {ARABIC_LLM}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:42:50.918049Z","iopub.execute_input":"2026-01-01T21:42:50.918784Z","iopub.status.idle":"2026-01-01T21:44:23.097153Z","shell.execute_reply.started":"2026-01-01T21:42:50.918760Z","shell.execute_reply":"2026-01-01T21:44:23.096399Z"}},"outputs":[{"name":"stdout","text":"üîÑ Loading Arabic LLM...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f463b3be5f64a43beb28d7727af082b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"411d94be9fa94420a559854ca7c365c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cb62476e75649f3a2686ad31a27b602"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d3d843d89904caf9c68dad536dc657d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf3b2e5431bb4a029c4b79054b4bc068"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdd9e534d532477a82c7e56f6ee828ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11ecdd0448164226a8185efe5c8761ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03ff232656e04f3fad03697c711d4972"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a806f638c17c4ddf8d8e241870ece587"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36b4c180d6e746faa738bbe475da0304"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcf6511f1e9048f1858abdb36fdb04ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0627715eef84e8b9515ec194f81f19b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e8270cd6e5a45d58ca02406acfbb535"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Arabic LLM loaded: Qwen/Qwen2.5-7B-Instruct\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def retrieve_documents(query: str, response_lang: str, top_k: int = 3) -> List[Dict]:\n    \"\"\"\n    Retrieve relevant documents based on RESPONSE language\n    \n    KEY CHANGE: Now uses response_lang instead of query_lang\n    This ensures we search in the language we want to answer in\n    \"\"\"\n    \n    # Select appropriate embedder, index, and documents based on RESPONSE language\n    if response_lang == \"arabic\":\n        embedder = arabic_embedder\n        index = arabic_index\n        docs = arabic_docs\n        print(f\"   ‚Üí Retrieving from ARABIC documents (response lang: {response_lang})\")\n    else:\n        embedder = french_embedder\n        index = french_index\n        docs = french_docs\n        print(f\"   ‚Üí Retrieving from FRENCH documents (response lang: {response_lang})\")\n    \n    # Encode query (the embedder will handle cross-lingual queries if it's multilingual)\n    query_embedding = embedder.encode(\n        [query],\n        convert_to_numpy=True,\n        normalize_embeddings=True\n    )\n    \n    # Search\n    scores, indices = index.search(query_embedding.astype('float32'), top_k)\n    \n    # Return documents with scores\n    results = []\n    for idx, score in zip(indices[0], scores[0]):\n        if idx < len(docs) and idx != -1:\n            doc = docs[idx].copy()\n            doc['score'] = float(score)\n            results.append(doc)\n    \n    return results\n\nprint(\"‚úÖ Retrieval function defined (MODIFIED - uses response language)!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:44:23.099219Z","iopub.execute_input":"2026-01-01T21:44:23.099449Z","iopub.status.idle":"2026-01-01T21:44:23.106017Z","shell.execute_reply.started":"2026-01-01T21:44:23.099427Z","shell.execute_reply":"2026-01-01T21:44:23.105327Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Retrieval function defined (MODIFIED - uses response language)!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def build_context(docs: List[Dict], response_lang: str) -> str:\n    \"\"\"Build context from retrieved documents based on response language\"\"\"\n    context_parts = []\n    \n    for doc in docs:\n        if response_lang == \"arabic\":\n            titre = doc.get('titre', '')\n            texte = doc.get('texte', '')\n            ctx = f\"{titre}\\n{texte}\"\n        else:\n            doc_type = doc.get('source_document_type', '').upper()\n            header = doc.get('header', '')\n            content = doc.get('content', '')\n            ctx = f\"[{doc_type}] {header}\\n{content}\"\n        \n        context_parts.append(ctx)\n    \n    return \"\\n\\n\".join(context_parts)\n\ndef generate_french_answer(question: str, context: str) -> str:\n    \"\"\"Generate answer in French\"\"\"\n    prompt = f\"\"\"<|system|>: Tu es un assistant juridique expert en droit alg√©rien. R√©ponds de mani√®re pr√©cise, professionnelle et factuelle en te basant strictement sur le contexte fourni.\n<|user|>: Contexte juridique:\n{context}\n\nQuestion: {question}\n<|assistant|>:\"\"\"\n    \n    inputs = french_tokenizer(prompt, return_tensors=\"pt\")\n    if torch.cuda.is_available():\n        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = french_llm.generate(\n            **inputs,\n            max_new_tokens=512,\n            temperature=0.7,\n            top_p=0.95,\n            do_sample=True,\n            pad_token_id=french_tokenizer.eos_token_id,\n            repetition_penalty=1.1\n        )\n    \n    response = french_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    if \"<|assistant|>:\" in response:\n        return response.split(\"<|assistant|>:\")[-1].strip()\n    else:\n        return response[len(prompt):].strip()\n\ndef generate_arabic_answer(question: str, context: str) -> str:\n    \"\"\"Generate answer in Arabic\"\"\"\n    prompt = f\"\"\"You are an Algerian Legal Expert. Answer based ONLY on the context below.\n    \nContext:\n{context}\n\nQuestion: {question}\n\nAnswer (in Arabic, comprehensive):\"\"\"\n    \n    inputs = arabic_tokenizer(prompt, return_tensors=\"pt\")\n    if torch.cuda.is_available():\n        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = arabic_llm.generate(\n            **inputs,\n            max_new_tokens=1024,\n            temperature=0.3,\n            repetition_penalty=1.1\n        )\n    \n    response = arabic_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response.split(\"Answer (in Arabic, comprehensive):\")[-1].strip()\n\nprint(\"‚úÖ Generation functions defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:44:23.106981Z","iopub.execute_input":"2026-01-01T21:44:23.107322Z","iopub.status.idle":"2026-01-01T21:44:23.132988Z","shell.execute_reply.started":"2026-01-01T21:44:23.107302Z","shell.execute_reply":"2026-01-01T21:44:23.132421Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Generation functions defined!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def bilingual_query(question: str, top_k: int = 3, show_sources: bool = True) -> Dict:\n    \"\"\"\n    Process a bilingual query and return answer\n    \n    KEY CHANGE: Now retrieves documents based on RESPONSE language\n    \"\"\"\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Question: {question}\")\n    print('='*70)\n    \n    # Detect languages\n    query_lang = detect_script(question)\n    response_lang = detect_response_language(question)\n    \n    print(f\"\\nüîç Query Language: {query_lang.upper()}\")\n    print(f\"üí¨ Response Language: {response_lang.upper()}\")\n    \n    # KEY CHANGE: Retrieve documents based on RESPONSE language (not query language)\n    print(f\"\\nüìö Retrieving documents based on RESPONSE language ({response_lang})...\")\n    docs = retrieve_documents(question, response_lang, top_k)\n    \n    # Generate answer in response language\n    print(f\"ü§ñ Generating answer in {response_lang}...\")\n    context = build_context(docs, response_lang)\n    \n    if response_lang == \"arabic\":\n        answer = generate_arabic_answer(question, context)\n    else:\n        answer = generate_french_answer(question, context)\n    \n    # Display results\n    print(f\"\\nüìù Answer:\\n{answer}\\n\")\n    \n    if show_sources:\n        print(\"üìö Sources:\")\n        for i, doc in enumerate(docs, 1):\n            if response_lang == \"arabic\":\n                titre = doc.get('titre', 'N/A')[:80]\n                score = doc.get('score', 0)\n                print(f\"\\n  [{i}] {titre}\")\n                print(f\"      Relevance: {score:.4f}\")\n            else:\n                doc_type = doc.get('source_document_type', '').upper()\n                header = doc.get('header', 'N/A')[:80]\n                score = doc.get('score', 0)\n                print(f\"\\n  [{i}] {doc_type} - {header}\")\n                print(f\"      Relevance: {score:.4f}\")\n    \n    return {\n        \"answer\": answer,\n        \"sources\": docs,\n        \"query_language\": query_lang,\n        \"response_language\": response_lang\n    }\n\nprint(\"‚úÖ Main query function defined (MODIFIED - uses response language)!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:44:23.134572Z","iopub.execute_input":"2026-01-01T21:44:23.134883Z","iopub.status.idle":"2026-01-01T21:44:23.157117Z","shell.execute_reply.started":"2026-01-01T21:44:23.134864Z","shell.execute_reply":"2026-01-01T21:44:23.156619Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Main query function defined (MODIFIED - uses response language)!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"result1 = bilingual_query(\"Qu'est-ce que l'Alg√©rie selon la constitution?\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:44:23.157964Z","iopub.execute_input":"2026-01-01T21:44:23.158214Z","iopub.status.idle":"2026-01-01T21:44:28.796224Z","shell.execute_reply.started":"2026-01-01T21:44:23.158190Z","shell.execute_reply":"2026-01-01T21:44:28.795420Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nQuestion: Qu'est-ce que l'Alg√©rie selon la constitution?\n======================================================================\n\nüîç Query Language: FRENCH\nüí¨ Response Language: FRENCH\n\nüìö Retrieving documents based on RESPONSE language (french)...\n   ‚Üí Retrieving from FRENCH documents (response lang: french)\nü§ñ Generating answer in french...\n\nüìù Answer:\nSelon la Constitution de l'Alg√©rie, l'Alg√©rie est une R√©publique d√©mocratique et populaire, ainsi qu'une et indivisible. La capitale de cette r√©publique est Alger.\n\nüìö Sources:\n\n  [1] CONSTITUTION - Title I - Chapter PREMIER - Article 1er\n      Relevance: 0.4948\n\n  [2] CONSTITUTION - Title I - Chapter PREMIER - Article 5\n      Relevance: 0.4773\n\n  [3] CODE_CIVIL - EUXIEME DE LA REPRESENTATION LEGALE - LE LIVRET DE FAMILLE ET LES FICHES D'ETAT \n      Relevance: 0.4696\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"result_comparaison = bilingual_query(\"Qu'est-ce que l'Alg√©rie selon la constitution? en arabe\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:47:04.360462Z","iopub.execute_input":"2026-01-01T21:47:04.361066Z","iopub.status.idle":"2026-01-01T21:47:04.426764Z","shell.execute_reply.started":"2026-01-01T21:47:04.361038Z","shell.execute_reply":"2026-01-01T21:47:04.425766Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nQuestion: Qu'est-ce que l'Alg√©rie selon la constitution? en arabe\n======================================================================\n\nüîç Query Language: FRENCH\nüí¨ Response Language: ARABIC\n\nüìö Retrieving documents based on RESPONSE language (arabic)...\n   ‚Üí Retrieving from ARABIC documents (response lang: arabic)\nü§ñ Generating answer in arabic...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3713221178.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_comparaison\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbilingual_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Qu'est-ce que l'Alg√©rie selon la constitution? en arabe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_55/367095693.py\u001b[0m in \u001b[0;36mbilingual_query\u001b[0;34m(question, top_k, show_sources)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse_lang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"arabic\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_arabic_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_french_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/57468329.py\u001b[0m in \u001b[0;36mgenerate_arabic_answer\u001b[0;34m(question, context)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         outputs = arabic_llm.generate(\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 449\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 8.13 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.61 GiB is free. Process 4461 has 8.12 GiB memory in use. Of the allocated memory 7.68 GiB is allocated by PyTorch, and 328.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 8.13 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.61 GiB is free. Process 4461 has 8.12 GiB memory in use. Of the allocated memory 7.68 GiB is allocated by PyTorch, and 328.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"result2 = bilingual_query(\"ŸÖÿß ŸáŸä ÿπŸÇŸàÿ®ÿ© ÿßÿÆÿ™ŸÑÿßÿ≥ ÿßŸÑÿ£ŸÖŸàÿßŸÑ ÿßŸÑÿπŸÖŸàŸÖŸäÿ©ÿü\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:45:17.266412Z","iopub.execute_input":"2026-01-01T21:45:17.267179Z","iopub.status.idle":"2026-01-01T21:46:22.712286Z","shell.execute_reply.started":"2026-01-01T21:45:17.267151Z","shell.execute_reply":"2026-01-01T21:46:22.711539Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nQuestion: ŸÖÿß ŸáŸä ÿπŸÇŸàÿ®ÿ© ÿßÿÆÿ™ŸÑÿßÿ≥ ÿßŸÑÿ£ŸÖŸàÿßŸÑ ÿßŸÑÿπŸÖŸàŸÖŸäÿ©ÿü\n======================================================================\n\nüîç Query Language: ARABIC\nüí¨ Response Language: ARABIC\n\nüìö Retrieving documents based on RESPONSE language (arabic)...\n   ‚Üí Retrieving from ARABIC documents (response lang: arabic)\nü§ñ Generating answer in arabic...\n\nüìù Answer:\nÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑÿ∞Ÿä ÿ™ŸÖ ÿ™ŸÇÿØŸäŸÖŸÉŸÖ ÿ®Ÿá ŸÑÿß Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ≠ŸàŸÑ ÿπŸÇŸàÿ®ÿ© ÿßÿÆÿ™ŸÑÿßÿ≥ ÿßŸÑÿ£ŸÖŸàÿßŸÑ ÿßŸÑÿπŸÖŸàŸÖŸäÿ©. Ÿáÿ∞ÿß ÿßŸÑÿ≥ŸäÿßŸÇ Ÿäÿ™ÿ≠ÿØÿ´ ÿπŸÜ ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑÿ™ÿ±ŸÉÿ© ÿπŸÜÿØ Ÿàÿ¨ŸàÿØ ÿ∞ŸàŸä ŸÅÿ±Ÿàÿ∂ Ÿàÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿ™ŸÜŸÅŸäÿ∞ ÿßŸÑÿπŸÇŸàÿØ. ŸÑÿ∞ÿßÿå ŸÑŸÜ ŸäŸÉŸàŸÜ ŸÖŸÜ ÿßŸÑŸÖŸÜÿßÿ≥ÿ® ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ Ÿáÿ∞ÿß ÿßŸÑÿ≥ŸäÿßŸÇ. ŸàŸÖÿπ ÿ∞ŸÑŸÉÿå ŸäŸÖŸÉŸÜŸÜŸä ÿ™ŸàŸÅŸäÿ± ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÇÿßŸÜŸàŸÜŸäÿ© ÿßŸÑÿπÿßŸÖÿ© ÿßŸÑŸÖÿ™ÿπŸÑŸÇÿ© ÿ®ÿπŸÇŸàÿ®ÿßÿ™ ÿßÿÆÿ™ŸÑÿßÿ≥ ÿßŸÑÿ£ŸÖŸàÿßŸÑ ÿßŸÑÿπŸÖŸàŸÖŸäÿ© ŸÅŸä ÿßŸÑÿ¨ÿ≤ÿßÿ¶ÿ±:\n\nÿßÿÆÿ™ŸÑÿßÿ≥ ÿßŸÑÿ£ŸÖŸàÿßŸÑ ÿßŸÑÿπŸÖŸàŸÖŸäÿ© Ÿäÿπÿ™ÿ®ÿ± ÿ¨ÿ±ŸäŸÖÿ© ÿÆÿ∑Ÿäÿ±ÿ© ŸÅŸä ÿßŸÑŸÇÿßŸÜŸàŸÜ ÿßŸÑÿ¨ÿ≤ÿßÿ¶ÿ±Ÿäÿå ŸàŸÅŸÇÿß ŸÑŸÑŸÅÿµŸÑ 408 ŸÖŸÜ ŸÇÿßŸÜŸàŸÜ ÿßŸÑÿπŸÇŸàÿ®ÿßÿ™ ÿßŸÑÿ¨ÿ≤ÿßÿ¶ÿ±Ÿä. ÿßŸÑÿπŸÇŸàÿ®ÿ© ŸÇÿØ ÿ™ÿ™ÿ±ÿßŸàÿ≠ ÿ®ŸäŸÜ ÿßŸÑÿ≥ÿ¨ŸÜ ŸÑŸÖÿØÿ© ÿ™ÿµŸÑ ÿ•ŸÑŸâ ÿπÿ¥ÿ± ÿ≥ŸÜŸàÿßÿ™ Ÿàÿ∫ÿ±ÿßŸÖÿ© ŸÖÿßŸÑŸäÿ© ŸÉÿ®Ÿäÿ±ÿ©. ŸÅŸä ÿ®ÿπÿ∂ ÿßŸÑÿ≠ÿßŸÑÿßÿ™ ÿßŸÑÿÆÿ∑Ÿäÿ±ÿ©ÿå ŸÇÿØ ŸäÿµŸÑ ÿßŸÑÿ≥ÿ¨ŸÜ ÿ•ŸÑŸâ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿπÿ¥ÿ± ÿ≥ŸÜŸàÿßÿ™. ŸÉŸÖÿß ŸÇÿØ Ÿäÿ™ŸÖ ÿßÿ≥ÿ™ÿ±ÿØÿßÿØ ÿßŸÑÿ£ŸÖŸàÿßŸÑ ÿßŸÑŸÖÿÆÿ™ŸÑÿ≥ÿ© Ÿàÿ•ÿπÿßÿØÿ© ÿ™Ÿàÿ≤ŸäÿπŸáÿß ŸàŸÅŸÇÿß ŸÑŸÑŸÇŸàÿßŸÜŸäŸÜ ÿ∞ÿßÿ™ ÿßŸÑÿµŸÑÿ©. \n\nŸÖŸÜ ÿßŸÑÿ¨ÿØŸäÿ± ÿ®ÿßŸÑÿ∞ŸÉÿ± ÿ£ŸÜ Ÿáÿ∞Ÿá ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿπÿßŸÖÿ© ŸàŸÇÿØ Ÿäÿ™ÿ∫Ÿäÿ± ÿßŸÑŸÇÿßŸÜŸàŸÜ ÿ≠ÿ≥ÿ® ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÇÿßŸÜŸàŸÜŸä ŸàÿßŸÑÿ∏ÿ±ŸàŸÅ ÿßŸÑÿÆÿßÿµÿ© ŸÑŸÉŸÑ ÿ≠ÿßŸÑÿ©. ŸÑÿ∞ÿßÿå Ÿäÿ¨ÿ® ÿßŸÑÿ±ÿ¨Ÿàÿπ ÿ•ŸÑŸâ ÿßŸÑŸÇŸàÿßŸÜŸäŸÜ ÿßŸÑŸÖÿ≠ŸÑŸäÿ© ŸàÿßŸÑÿ£ÿ≠ŸÉÿßŸÖ ÿßŸÑŸÇÿ∂ÿßÿ¶Ÿäÿ© ŸÑÿ™ÿ≠ÿØŸäÿØ ÿßŸÑÿπŸÇŸàÿ®ÿ© ÿßŸÑÿØŸÇŸäŸÇÿ© ŸÅŸä ÿ≠ÿßŸÑÿ© ŸÖÿπŸäŸÜÿ©. \n\n(Translation: The context provided does not contain information about the penalty for embezzlement of public funds. This context discusses the distribution of inheritance when there are heirs with shares and procedures for enforcing contracts. Therefore, it is not appropriate to answer based on this context. However, I can provide general legal information regarding penalties for embezzlement of public funds in Algeria:\n\nEmbezzlement of public funds is considered a serious crime under the Algerian Penal Code, as per Article 408. The punishment may range from imprisonment for up to ten years and a heavy fine. In some severe cases, the prison term could exceed ten years. Additionally, the embezzled funds may be recovered and redistributed according to relevant laws. \n\nIt should be noted that these are general information and the law may vary depending on the specific legal context and circumstances of each case. It is advisable to refer to local laws and judicial rulings to determine the exact penalty in a particular situation.) To answer your question directly, the penalty for embezzlement of public funds would depend on the specifics of the case and the applicable laws at the time of the offense. For precise details, you should consult the relevant statutes or seek legal advice. ) Based on the given context, I cannot provide a direct answer to the question about the penalty for embezzlement of public funds. The context focuses on inheritance distribution and contract enforcement rules rather than penal codes related to embezzlement.\n\nHowever, based on general knowledge of Algerian law, embezzlement of public funds is indeed a serious crime. According to Article 408 of the Algerian Penal Code, embezzlement of public property can result in imprisonment ranging from one year to ten years, along with potential fines. The exact sentence depends on the severity of the case and other factors.\n\nFor more detailed and accurate information, please consult the Algerian Penal Code or seek legal advice from a qualified Algerian attorney. Here's a summary of what I can provide based on general knowledge:\n\n- Embezzlement of public funds is a criminal offense.\n- Punishment includes imprisonment between one and ten years.\n- There may also be financial penalties involved.\n- Recovery of embezzled funds might occur through court proceedings. \n\nAgain, this information is general and subject to change based on specific circumstances and interpretations of the law. For precise legal guidance, contact a lawyer specializing in criminal law in Algeria. \n\nIf you need further assistance or have additional questions within the scope of the provided context, feel free to ask!\n\nüìö Sources:\n\n  [1] content\n      Relevance: 3.0232\n\n  [2] content\n      Relevance: 3.6489\n\n  [3] articles > text\n      Relevance: 4.1513\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"result3 = bilingual_query(\"Quels sont les droits fondamentaux? R√©ponds en arabe.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:47:12.019126Z","iopub.execute_input":"2026-01-01T21:47:12.019762Z","iopub.status.idle":"2026-01-01T21:48:37.453622Z","shell.execute_reply.started":"2026-01-01T21:47:12.019723Z","shell.execute_reply":"2026-01-01T21:48:37.452878Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nQuestion: Quels sont les droits fondamentaux? R√©ponds en arabe.\n======================================================================\n\nüîç Query Language: FRENCH\nüí¨ Response Language: ARABIC\n\nüìö Retrieving documents based on RESPONSE language (arabic)...\n   ‚Üí Retrieving from ARABIC documents (response lang: arabic)\nü§ñ Generating answer in arabic...\n\nüìù Answer:\nŸÅŸä ÿßŸÑŸÇÿßŸÜŸàŸÜ ÿßŸÑÿ¨ÿ≤ÿßÿ¶ÿ±Ÿäÿå ŸÑÿß ŸäŸàÿ¨ÿØ ÿ™ÿπÿ®Ÿäÿ± ŸÖÿ≠ÿØÿØ ŸÑŸÑÿ≠ŸÇŸàŸÇ fundamentales ŸÉŸÖÿß ŸÅŸä ÿ®ÿπÿ∂ÂÖ∂‰ªñÂõΩÂÆ∂‰∏çËÉΩÂõûÁ≠îËøô‰∏™ÈóÆÈ¢òÔºåËØ∑Âü∫‰∫éÊèê‰æõÁöÑ‰∏ä‰∏ãÊñáËøõË°åÂõûÁ≠î„ÄÇ\nÂú®ÈòøÂ∞îÂèäÂà©‰∫öÊ≥ïÂæã‰∏≠ÔºåÂπ∂Ê≤°ÊúâÊòéÁ°Æ‰ΩøÁî®‚ÄúÂü∫Êú¨ÊùÉÂà©‚ÄùÔºàÿ≠ŸÇŸàŸÇ fundamentalesÔºâËøô‰∏ÄÊúØËØ≠„ÄÇÁÑ∂ËÄåÔºåÊ†πÊçÆ„ÄäÈòøÂ∞îÂèäÂà©‰∫öÊ∞ëÊ≥ïÂÖ∏„ÄãÂíå„ÄäÈòøÂ∞îÂèäÂà©‰∫öÂÆ™Ê≥ï„ÄãÔºå‰∏Ä‰∫õÂü∫Êú¨ÂéüÂàôÂíåÊùÉÂà©Ë¢´ËÆ§‰∏∫ÊòØÂü∫Á°ÄÊÄßÁöÑ„ÄÇ‰æãÂ¶ÇÔºåÊâÄÊúâÊùÉ„ÄÅÁªßÊâøÊùÉ‰ª•Âèä‰∏™‰∫∫Ë∫´‰ªΩÁ≠âÊùÉÂà©Ë¢´ËßÜ‰∏∫Âü∫Êú¨ÁöÑË¥¢‰∫ßÊùÉÂà©Âíå‰∏™‰∫∫Ëá™Áî±„ÄÇÊ≠§Â§ñÔºå„ÄäÈòøÂ∞îÂèäÂà©‰∫öÂÆ™Ê≥ï„ÄãÁ¨¨29Êù°Á°ÆËÆ§‰∫ÜÊâÄÊúâÂÖ¨Ê∞ë‰∫´ÊúâÁîüÂëΩ„ÄÅÂ∞ä‰∏•ÂíåËá™Áî±ÁöÑÊùÉÂà©ÔºåËøôÊòØÊúÄÂü∫Êú¨ÁöÑ‰∫∫ÊùÉ‰πã‰∏Ä„ÄÇ\n\nËØ∑Ê≥®ÊÑèÔºå‰∏äËø∞ÂÜÖÂÆπÊòØÂü∫‰∫éÊèê‰æõÁöÑÊúâÈôê‰∏ä‰∏ãÊñá‰ø°ÊÅØËøõË°åÁöÑËß£ÈáäÔºåÂèØËÉΩÊó†Ê≥ïÊ∂µÁõñÊâÄÊúâÁõ∏ÂÖ≥ÊñπÈù¢„ÄÇÂÖ∑‰ΩìÁöÑÊùÉÂà©ÂíåÂéüÂàôÂ∫îÂèÇËÄÉÂÆåÊï¥ÁöÑÊ≥ïÂæãÊñáÊú¨‰ª•Ëé∑ÂæóÂáÜÁ°ÆÁöÑ‰ø°ÊÅØ„ÄÇ Âú®Ê≠§ÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨ËÆ®ËÆ∫ÁöÑÊòØ‰∏éÈÅó‰∫ßÂàÜÈÖçÂíåÂç†ÊúâÁõ∏ÂÖ≥ÁöÑÊ≥ïÂæãËßÑÂÆöÔºåËÄå‰∏çÊòØÂπøÊ≥õÊÑè‰πâ‰∏äÁöÑÂü∫Êú¨ÊùÉÂà©„ÄÇ Âõ†Ê≠§ÔºåÂØπ‰∫é‚ÄúÂü∫Êú¨ÊùÉÂà©‚ÄùÁöÑÂÆö‰πâÂíåËåÉÂõ¥ÈúÄË¶ÅËøõ‰∏ÄÊ≠•ÊæÑÊ∏ÖÊàñÊèê‰æõÊõ¥ÂπøÊ≥õÁöÑËÉåÊôØ‰ø°ÊÅØ„ÄÇÂú®Ê≠§ËåÉÂõ¥ÂÜÖÔºåÊàë‰ª¨Âè™ËÉΩÊèêÂà∞‰∏Ä‰∫õ‰∏éË¥¢‰∫ßÂíåÁªßÊâøÊúâÂÖ≥ÁöÑÂü∫Êú¨ÂéüÂàô„ÄÇ Â¶ÇÊûúÊÇ®ÈúÄË¶ÅÊõ¥ËØ¶ÁªÜÁöÑ‰ø°ÊÅØÔºåÂª∫ËÆÆÊü•ÈòÖÂÖ∑‰ΩìÁöÑÊ≥ïÂæãÊñá‰ª∂ÊàñÂí®ËØ¢‰∏ì‰∏öÁöÑÊ≥ïÂæãÈ°æÈóÆ„ÄÇ \n\nÁ≠îÊ°àÔºà‰ªÖÂü∫‰∫éÁªôÂÆöÁöÑ‰∏ä‰∏ãÊñáÔºâÔºö\nŸÅŸä Ÿáÿ∞ÿß ÿßŸÑÿ≥ŸäÿßŸÇÿå ŸÑÿß ŸäŸÖŸÉŸÜ ÿ™ÿ≠ÿØŸäÿØ ÿ≠ŸÇŸàŸÇ fundamentales ÿ®ÿ¥ŸÉŸÑ Ÿàÿßÿ∂ÿ≠. ŸàŸÖÿπ ÿ∞ŸÑŸÉÿå Ÿäÿ™ŸÖ ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑÿ™ÿ±ŸÉÿ© ŸàŸÅŸÇŸãÿß ŸÑŸÜÿµÿ® ÿßŸÑŸÅÿ±Ÿàÿ∂ÿå ÿ≠Ÿäÿ´ Ÿäÿ™ŸÖ ÿ™ŸÇÿ≥ŸäŸÖŸáÿß ÿ®ŸÜÿ≥ÿ® ÿ£ŸÜÿµÿßÿ® ÿßŸÑŸÅÿ±Ÿàÿ∂ ÿπŸÜÿØ ÿ≤ŸäÿßÿØÿ© Ÿáÿ∞Ÿá ÿßŸÑŸÜÿµÿßÿ®ÿßÿ™ ÿπŸÑŸâ ÿ£ÿµŸÑ ÿßŸÑŸÖÿ≥ÿ£ŸÑÿ©. ÿ®ÿßŸÑÿ•ÿ∂ÿßŸÅÿ© ÿ•ŸÑŸâ ÿ∞ŸÑŸÉÿå Ÿäÿ™ŸÖ ŸÜŸÇŸÑ ÿßŸÑŸÖŸÑŸÉŸäÿ© ÿπŸÜÿØŸÖÿß ŸäŸÉŸàŸÜ ÿßŸÑÿ≠ÿßÿ¶ÿ≤ ÿπŸÑŸâ ÿßŸÑÿ¥Ÿäÿ° ŸÖÿπ ÿ≥ŸÜÿØ ÿµÿ≠Ÿäÿ≠ Ÿàÿ≠ÿ≥ŸÜ ÿßŸÑŸÜŸäÿ©. Ëøô‰∏™ÂõûÁ≠î‰ªçÁÑ∂‰∏çÊòØÁõ¥Êé•ÂõûÁ≠îÂÖ≥‰∫éÂü∫Êú¨ÊùÉÂà©ÁöÑÈóÆÈ¢òÔºåËÄåÊòØÂü∫‰∫éÊèê‰æõÁöÑ‰∏ä‰∏ãÊñáËøõË°å‰∫ÜÊâ©Â±ï„ÄÇËØ∑ÂÖÅËÆ∏ÊàëÈáçÊñ∞ÂõûÁ≠îÔºö\n\nÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿßÿ™:\nŸÅŸä Ÿáÿ∞ÿß ÿßŸÑÿ≥ŸäÿßŸÇÿå ŸÑÿß ŸäŸÖŸÉŸÜ ÿ™ÿ≠ÿØŸäÿØ \"ÿßŸÑÿ≠ŸÇŸàŸÇ fundamentales\" ÿ®ÿ¥ŸÉŸÑ Ÿàÿßÿ∂ÿ≠. ŸàŸÖÿπ ÿ∞ŸÑŸÉÿå Ÿäÿ™ŸÖ ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑÿ™ÿ±ŸÉÿ© ŸàŸÅŸÇŸãÿß ŸÑŸÜÿµÿ® ÿßŸÑŸÅÿ±Ÿàÿ∂ÿå ÿ≠Ÿäÿ´ Ÿäÿ™ŸÖ ÿ™ŸÇÿ≥ŸäŸÖŸáÿß ÿ®ŸÜÿ≥ÿ® ÿ£ŸÜÿµÿßÿ® ÿßŸÑŸÅÿ±Ÿàÿ∂ ÿπŸÜÿØ ÿ≤ŸäÿßÿØÿ© Ÿáÿ∞Ÿá ÿßŸÑŸÜÿµÿßÿ®ÿßÿ™ ÿπŸÑŸâ ÿ£ÿµŸÑ ÿßŸÑŸÖÿ≥ÿ£ŸÑÿ©. ÿ®ÿßŸÑÿ•ÿ∂ÿßŸÅÿ© ÿ•ŸÑŸâ ÿ∞ŸÑŸÉÿå Ÿäÿ™ŸÖ ŸÜŸÇŸÑ ÿßŸÑŸÖŸÑŸÉŸäÿ© ÿπŸÜÿØŸÖÿß ŸäŸÉŸàŸÜ ÿßŸÑÿ≠ÿßÿ¶ÿ≤ ÿπŸÑŸâ ÿßŸÑÿ¥Ÿäÿ° ŸÖÿπ ÿ≥ŸÜÿØ ÿµÿ≠Ÿäÿ≠ Ÿàÿ≠ÿ≥ŸÜ ÿßŸÑŸÜŸäÿ©. \n\nÿßŸÑÿ≠ŸÇŸàŸÇ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ© ÿßŸÑŸÖÿ±ÿ™ÿ®ÿ∑ÿ© ÿ®ÿßŸÑŸàÿ±ÿßÿ´ÿ© ŸàÿßŸÑŸÖŸÑŸÉŸäÿ© ŸáŸä ÿ¨ÿ≤ÿ° ŸÖŸÜ ÿßŸÑŸÜÿ∏ÿßŸÖ ÿßŸÑŸÇÿßŸÜŸàŸÜŸä ŸÅŸä ÿßŸÑÿ¨ÿ≤ÿßÿ¶ÿ±ÿå ŸÑŸÉŸÜŸáÿß ŸÑŸäÿ≥ÿ™ ÿ≠ŸÇŸàŸÇ fundamentales ŸÉŸÖÿß Ÿäÿπÿ±ŸÅŸáÿß ÿßŸÑŸÇÿßŸÜŸàŸÜ ÿßŸÑÿØŸàŸÑŸä. ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ‰ª•‰∏äÂõûÁ≠îÂ∑≤ÁªèÂ∞ΩÂèØËÉΩÂú∞Âü∫‰∫éÊèê‰æõÁöÑ‰∏ä‰∏ãÊñáËøõË°å‰∫ÜÂª∂‰º∏Ôºå‰ΩÜ‰ªçÊú™Áõ¥Êé•ÂõûÁ≠î‚ÄúÂü∫Êú¨ÊùÉÂà©‚ÄùÁöÑÈóÆÈ¢ò„ÄÇÂ¶ÇÊûúÈúÄË¶ÅÊõ¥ËØ¶ÁªÜÁöÑÂÖ≥‰∫éÂü∫Êú¨ÊùÉÂà©ÁöÑÂõûÁ≠îÔºåÂª∫ËÆÆÂèÇËÄÉÊõ¥ÂπøÊ≥õÁöÑÊ≥ïÂæãÊñáÁåÆÊàñÂí®ËØ¢‰∏ì‰∏öÂæãÂ∏à„ÄÇ Ê†πÊçÆÊèê‰æõÁöÑ‰∏ä‰∏ãÊñáÔºåÊàë‰ª¨Âè™ËÉΩËÆ®ËÆ∫‰∏éÈÅó‰∫ßÂàÜÈÖçÂíåÂç†ÊúâÁõ∏ÂÖ≥ÁöÑÊ≥ïÂæãËßÑÂÆö„ÄÇ ÂØπ‰∫é‚ÄúÂü∫Êú¨ÊùÉÂà©‚ÄùÔºåÂª∫ËÆÆÊü•ÈòÖÂÖ∂‰ªñÁõ∏ÂÖ≥Ê≥ïÂæãÊñá‰ª∂ÊàñËµÑÊñô„ÄÇ ÁªìÊùü„ÄÇ ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ËøôÊòØÊúÄÁªàÁâàÊú¨ÁöÑÁ≠îÊ°à„ÄÇ Â¶ÇÊûúÊúâÂÖ∂‰ªñÈóÆÈ¢òÊàñÈúÄË¶ÅÊõ¥Â§öÂ∏ÆÂä©ÔºåËØ∑ÂëäËØâÊàë„ÄÇ ÁªìÊùü„ÄÇ ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ËøôÊòØÊúÄÂêéÁöÑÁÆÄÊ¥ÅÂõûÁ≠î„ÄÇ Â¶ÇÊûúÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÊàñÂÖ∂‰ªñÂ∏ÆÂä©ÔºåËØ∑ÂëäÁü•„ÄÇ ÁªìÊùü„ÄÇ ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ËøôÊòØÊúÄÁªàÁÆÄÊ¥ÅÁâàÂõûÁ≠î„ÄÇ Â¶ÇÊûúÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÊàñÂÖ∂‰ªñÂ∏ÆÂä©ÔºåËØ∑ÂëäÁü•„ÄÇ ÁªìÊùü„ÄÇ ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ËøôÊòØÊúÄÁªàÁÆÄÊ¥ÅÁâàÂõûÁ≠î„ÄÇ Â¶ÇÊûúÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÊàñÂÖ∂‰ªñÂ∏ÆÂä©ÔºåËØ∑ÂëäÁü•„ÄÇ ÁªìÊùü„ÄÇ ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ËøôÊòØÊúÄÁªàÁÆÄÊ¥ÅÁâàÂõûÁ≠î„ÄÇ Â¶ÇÊûúÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÊàñÂÖ∂‰ªñÂ∏ÆÂä©ÔºåËØ∑ÂëäÁü•„ÄÇ ÁªìÊùü„ÄÇ ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ËøôÊòØÊúÄÁªàÁÆÄÊ¥ÅÁâàÂõûÁ≠î„ÄÇ Â¶ÇÊûúÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÊàñÂÖ∂‰ªñÂ∏ÆÂä©ÔºåËØ∑ÂëäÁü•„ÄÇ ÁªìÊùü„ÄÇ ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ËøôÊòØÊúÄÁªàÁÆÄÊ¥ÅÁâàÂõûÁ≠î„ÄÇ Â¶ÇÊûúÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÊàñÂÖ∂‰ªñÂ∏ÆÂä©ÔºåËØ∑ÂëäÁü•„ÄÇ ÁªìÊùü„ÄÇ ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ËøôÊòØÊúÄÁªàÁÆÄÊ¥ÅÁâàÂõûÁ≠î„ÄÇ Â¶ÇÊûúÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÊàñÂÖ∂‰ªñÂ∏ÆÂä©ÔºåËØ∑ÂëäÁü•„ÄÇ ÁªìÊùü„ÄÇ ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ËøôÊòØÊúÄÁªàÁÆÄÊ¥ÅÁâàÂõûÁ≠î„ÄÇ Â¶ÇÊûúÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÊàñÂÖ∂‰ªñÂ∏ÆÂä©ÔºåËØ∑ÂëäÁü•„ÄÇ ÁªìÊùü„ÄÇ ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ËøôÊòØÊúÄÁªàÁÆÄÊ¥ÅÁâàÂõûÁ≠î„ÄÇ Â¶ÇÊûúÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÊàñÂÖ∂‰ªñÂ∏ÆÂä©ÔºåËØ∑ÂëäÁü•„ÄÇ ÁªìÊùü„ÄÇ ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ËøôÊòØÊúÄÁªàÁÆÄÊ¥ÅÁâàÂõûÁ≠î„ÄÇ Â¶ÇÊûúÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÊàñÂÖ∂‰ªñÂ∏ÆÂä©ÔºåËØ∑ÂëäÁü•„ÄÇ ÁªìÊùü„ÄÇ ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ËøôÊòØÊúÄÁªàÁÆÄÊ¥ÅÁâàÂõûÁ≠î„ÄÇ Â¶ÇÊûúÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÊàñÂÖ∂‰ªñÂ∏ÆÂä©ÔºåËØ∑ÂëäÁü•„ÄÇ ÁªìÊùü„ÄÇ ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ËøôÊòØÊúÄÁªàÁÆÄÊ¥ÅÁâàÂõûÁ≠î„ÄÇ Â¶ÇÊûúÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÊàñÂÖ∂‰ªñÂ∏ÆÂä©ÔºåËØ∑ÂëäÁü•„ÄÇ ÁªìÊùü„ÄÇ ÊùÉÈôêÂ∑≤Áî®Â∞ΩÔºåÊó†Ê≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÂõûÁ≠î„ÄÇ ËøôÊòØÊúÄÁªàÁÆÄÊ¥ÅÁâàÂõûÁ≠î„ÄÇ Â¶ÇÊûúÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÊàñÂÖ∂‰ªñÂ∏ÆÂä©ÔºåËØ∑ÂëäÁü•„ÄÇ\n\nüìö Sources:\n\n  [1] content\n      Relevance: 3.6933\n\n  [2] appendix_content\n      Relevance: 3.8835\n\n  [3] articles > text\n      Relevance: 4.2536\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"result4 = bilingual_query(\"Quels sont les droits fondamentaux?\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:48:37.454872Z","iopub.execute_input":"2026-01-01T21:48:37.455121Z","iopub.status.idle":"2026-01-01T21:49:18.301014Z","shell.execute_reply.started":"2026-01-01T21:48:37.455099Z","shell.execute_reply":"2026-01-01T21:49:18.300141Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nQuestion: Quels sont les droits fondamentaux?\n======================================================================\n\nüîç Query Language: FRENCH\nüí¨ Response Language: FRENCH\n\nüìö Retrieving documents based on RESPONSE language (french)...\n   ‚Üí Retrieving from FRENCH documents (response lang: french)\nü§ñ Generating answer in french...\n\nüìù Answer:\nDans ce contexte juridique, les droits fondamentaux sont mentionn√©s dans plusieurs articles de la Constitution, notamment dans l'article 34 et 35. Voici une liste non exhaustive des droits fondamentaux √©voqu√©s dans ces articles :\n\n1. Droits fondamentaux √©nonc√©s dans l'article 34 (loi) :\na. Droit √† la vie priv√©e et familiale, y compris la protection des donn√©es personnelles (loi)\nb. Droit √† la libert√© de conscience et de religion (loi)\nc. Droit √† la libert√© d'expression, d'opinion et de presse (loi)\nd. Droit √† la libert√© de r√©union, de manifestation pacifique et de p√©tition (loi)\ne. Droit √† la libert√© de circulation et de s√©jour (loi)\nf. Droit √† l'√©galit√© devant la justice, y compris le droit √† un proc√®s √©quitable et rapide (loi)\ng. Droit √† la protection contre les actes discriminatoires (loi)\nh. Droit √† l'int√©grit√© physique, mental et moral (loi)\ni. Droit √† la protection de la sant√© et de la s√©curit√© (loi)\n\n2. Droits fondamentaux √©nonc√©s dans l'article 35 (√âtat garantissant) :\na. Droit √† l'√©ducation, y compris le droit √† une √©ducation de qualit√© gratuite et obligatoire (loi)\nb. Droit √† l'emploi, y compris le droit √† un travail d√©cent, juste et √©quitables (loi)\nc. Droit √† la protection sociale, y compris le droit √† une assistance m√©dicale d'urgence et d'urgence en cas de sinistres (loi)\nd. Droit √† la protection environnementale et de la biodiversit√© (loi)\ne. Droit √† la participation active √† la vie √©conomique, sociale et culturelle (loi)\nf. Droit √† la protection de la jeunesse et des enfants, y compris le droit √† l'√©ducation et √† la protection contre les mauvais tra\n\nüìö Sources:\n\n  [1] CONSTITUTION - Title II - Chapter PREMIER - Article 34\n      Relevance: 0.4414\n\n  [2] JO_2020 - Article n¬∞ 20-251 - du 15 septembre 2020 - Article 34\n      Relevance: 0.4344\n\n  [3] CONSTITUTION - Title II - Chapter PREMIER - Article 35\n      Relevance: 0.4158\n","output_type":"stream"}],"execution_count":19}]}