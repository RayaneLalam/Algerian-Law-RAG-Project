{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14293395,"sourceType":"datasetVersion","datasetId":9123950}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:38:17.346172Z","iopub.execute_input":"2025-12-25T19:38:17.346552Z","iopub.status.idle":"2025-12-25T19:38:17.633843Z","shell.execute_reply.started":"2025-12-25T19:38:17.346519Z","shell.execute_reply":"2025-12-25T19:38:17.633042Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/faiss-data/algerian_legal(joconstitutionpenalecivilcommercefamille) embedder_ dangvantuan-sentence-camembert-large.faiss\n/kaggle/input/faiss-data/algerian_legal(joconstitutionpenalecivilcommercefamille) embedder_ dangvantuan-sentence-camembert-large_meta.json\n/kaggle/input/faiss-data/algerian_legal(joconstitutionpenalecivilcommercefamille) embedder_ dangvantuan-sentence-camembert-large_docs.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q transformers torch sentence-transformers faiss-cpu accelerate bitsandbytes sacremoses sentencepiece langdetect\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:38:17.635166Z","iopub.execute_input":"2025-12-25T19:38:17.635592Z","iopub.status.idle":"2025-12-25T19:38:29.089314Z","shell.execute_reply.started":"2025-12-25T19:38:17.635570Z","shell.execute_reply":"2025-12-25T19:38:29.088577Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import json\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\nfrom typing import List, Dict, Tuple, Optional\nfrom pathlib import Path\nimport warnings\nfrom langdetect import detect, LangDetectException\nwarnings.filterwarnings('ignore')\n\nprint(\"‚úÖ All libraries imported successfully!\")\nprint(f\"‚úÖ PyTorch version: {torch.__version__}\")\nprint(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:38:29.090589Z","iopub.execute_input":"2025-12-25T19:38:29.090855Z","iopub.status.idle":"2025-12-25T19:39:00.293370Z","shell.execute_reply.started":"2025-12-25T19:38:29.090828Z","shell.execute_reply":"2025-12-25T19:39:00.292591Z"}},"outputs":[{"name":"stderr","text":"2025-12-25 19:38:42.973925: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766691523.197137      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766691523.259221      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766691523.819244      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766691523.819281      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766691523.819284      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766691523.819287      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ All libraries imported successfully!\n‚úÖ PyTorch version: 2.8.0+cu126\n‚úÖ CUDA available: True\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# ARABIC DISPLAY FIX - Add this cell\n# ============================================================================\nimport sys\nimport locale\nimport unicodedata\n\n# Force UTF-8 encoding for stdout\ntry:\n    if sys.stdout.encoding != 'utf-8':\n        sys.stdout.reconfigure(encoding='utf-8')\nexcept:\n    pass\n\n# Set locale\ntry:\n    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\nexcept:\n    pass\n\ndef normalize_arabic_text(text: str) -> str:\n    \"\"\"Normalize Arabic text for proper display\"\"\"\n    if not text:\n        return text\n    # Normalize Unicode to ensure proper Arabic rendering\n    text = unicodedata.normalize('NFC', text)\n    return text\n\ndef print_arabic(text: str, prefix: str = \"\"):\n    \"\"\"Print Arabic text with proper encoding and BiDi markers\"\"\"\n    if not text:\n        return\n    \n    # Normalize the text\n    normalized = normalize_arabic_text(text)\n    \n    # Add Unicode BiDi markers for proper right-to-left display\n    RLE = '\\u202B'  # Right-to-Left Embedding\n    PDF = '\\u202C'  # Pop Directional Formatting\n    \n    # Check if text contains Arabic characters\n    has_arabic = any('\\u0600' <= char <= '\\u06FF' or '\\u0750' <= char <= '\\u077F' for char in normalized)\n    \n    if has_arabic:\n        # Wrap Arabic text with BiDi markers\n        display_text = f\"{RLE}{normalized}{PDF}\"\n    else:\n        display_text = normalized\n    \n    # Print with explicit encoding\n    try:\n        print(f\"{prefix}{display_text}\", flush=True)\n    except UnicodeEncodeError:\n        # Fallback: try printing without BiDi markers\n        print(f\"{prefix}{normalized}\", flush=True)\n\n# Test Arabic display\nprint(\"‚úÖ Arabic display functions loaded!\")\nprint(\"Testing Arabic display: \", end=\"\")\nprint_arabic(\"ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ŸÉ - Arabic text should display correctly\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:39:00.295239Z","iopub.execute_input":"2025-12-25T19:39:00.295879Z","iopub.status.idle":"2025-12-25T19:39:00.304080Z","shell.execute_reply.started":"2025-12-25T19:39:00.295856Z","shell.execute_reply":"2025-12-25T19:39:00.303403Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Arabic display functions loaded!\nTesting Arabic display: ‚Ä´ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ŸÉ - Arabic text should display correctly‚Ä¨\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Paths (Update these to match your uploaded files)\nINDEX_PATH = \"/kaggle/input/faiss-data/algerian_legal(joconstitutionpenalecivilcommercefamille) embedder_ dangvantuan-sentence-camembert-large.faiss\"\nDOCS_PATH = INDEX_PATH.replace('.faiss', '_docs.json')\nMETA_PATH = INDEX_PATH.replace('.faiss', '_meta.json')\n\n# Models\nEMBEDDING_MODEL = \"dangvantuan/sentence-camembert-large\"\nLLM_MODEL = \"bofenghuang/vigogne-2-7b-chat\"\n\n# Translation Model - Best for Arabic-French legal translation\n# NLLB-200 is Meta's state-of-the-art multilingual model, excellent for Arabic-French\nTRANSLATION_MODEL = \"facebook/nllb-200-distilled-600M\"  # Good balance of quality and speed\n# Alternative: \"facebook/nllb-200-1.3B\" for better quality but slower\n# Alternative: \"Helsinki-NLP/opus-mt-ar-fr\" and \"Helsinki-NLP/opus-mt-fr-ar\" (lighter but less accurate)\n\n# Settings\nUSE_4BIT_QUANTIZATION = True\nTOP_K_RETRIEVAL = 3\n\nprint(\"‚úÖ Configuration set!\")\nprint(f\"  Embedding Model: {EMBEDDING_MODEL}\")\nprint(f\"  LLM Model: {LLM_MODEL}\")\nprint(f\"  Translation Model: {TRANSLATION_MODEL}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:39:00.305172Z","iopub.execute_input":"2025-12-25T19:39:00.305439Z","iopub.status.idle":"2025-12-25T19:39:00.325689Z","shell.execute_reply.started":"2025-12-25T19:39:00.305419Z","shell.execute_reply":"2025-12-25T19:39:00.325002Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Configuration set!\n  Embedding Model: dangvantuan/sentence-camembert-large\n  LLM Model: bofenghuang/vigogne-2-7b-chat\n  Translation Model: facebook/nllb-200-distilled-600M\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(\"Loading translation model...\")\n\ntranslation_tokenizer = AutoTokenizer.from_pretrained(\n    TRANSLATION_MODEL,\n    src_lang=\"fra_Latn\",\n    tgt_lang=\"ara_Arab\"\n)\ntranslation_model = AutoModelForSeq2SeqLM.from_pretrained(\n    TRANSLATION_MODEL,\n    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n    device_map=\"auto\" if torch.cuda.is_available() else None\n)\n\n# Get language token IDs for NLLB\ndef get_lang_token_id(tokenizer, lang_code):\n    \"\"\"Get the token ID for a language code\"\"\"\n    # For NLLB models, language codes are special tokens\n    try:\n        return tokenizer.convert_tokens_to_ids(lang_code)\n    except:\n        # Alternative method\n        return tokenizer.lang_code_to_id[lang_code] if hasattr(tokenizer, 'lang_code_to_id') else None\n\n# Store language token IDs\nLANG_TOKEN_IDS = {\n    'ara_Arab': get_lang_token_id(translation_tokenizer, 'ara_Arab'),\n    'fra_Latn': get_lang_token_id(translation_tokenizer, 'fra_Latn')\n}\n\nprint(f\"‚úÖ Translation model loaded: {TRANSLATION_MODEL}\")\nprint(f\"  Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\nprint(f\"  Language tokens: ara_Arab={LANG_TOKEN_IDS['ara_Arab']}, fra_Latn={LANG_TOKEN_IDS['fra_Latn']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:39:00.326526Z","iopub.execute_input":"2025-12-25T19:39:00.326701Z","iopub.status.idle":"2025-12-25T19:39:19.518906Z","shell.execute_reply.started":"2025-12-25T19:39:00.326685Z","shell.execute_reply":"2025-12-25T19:39:19.518253Z"}},"outputs":[{"name":"stdout","text":"Loading translation model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc40990688454ee9b447d0fff871d468"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4ac58cfb6c4493ea2cc956464a13122"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bde919e914ce46e5b4359da9bb3934b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a10932fd15cf4b5f8487dcb42d99236d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a1a233814c649399b6a851d7c4d107f"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec63e2a97fdc4ffa91f7053dcc6464ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3f65807f4e94088a1a02f218dafd072"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"913b0e41a28f42ef92d894c7192b302a"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Translation model loaded: facebook/nllb-200-distilled-600M\n  Device: cuda\n  Language tokens: ara_Arab=3, fra_Latn=256057\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def detect_language(text: str) -> str:\n    \"\"\"\n    Detect if text is in Arabic or French\n    \n    Args:\n        text: Input text\n        \n    Returns:\n        'ar' for Arabic, 'fr' for French, 'unknown' for others\n    \"\"\"\n    try:\n        # Remove extra whitespace\n        text = text.strip()\n        \n        # Check for Arabic characters\n        arabic_chars = sum(1 for c in text if '\\u0600' <= c <= '\\u06FF')\n        total_chars = len([c for c in text if c.isalpha()])\n        \n        if total_chars == 0:\n            return 'unknown'\n        \n        # If more than 30% Arabic characters, it's Arabic\n        if arabic_chars / total_chars > 0.3:\n            return 'ar'\n        \n        # Use langdetect for French\n        detected = detect(text)\n        if detected == 'fr':\n            return 'fr'\n        elif detected == 'ar':\n            return 'ar'\n        else:\n            # Default to French if uncertain (since data is in French)\n            return 'fr' if arabic_chars == 0 else 'ar'\n            \n    except LangDetectException:\n        return 'unknown'\n\ndef detect_output_language_intent(text: str) -> Optional[str]:\n    \"\"\"\n    Detect if user explicitly requests output in specific language\n    \n    Args:\n        text: User's question\n        \n    Returns:\n        'ar' if Arabic output requested, 'fr' if French requested, None otherwise\n    \"\"\"\n    text_lower = text.lower()\n    \n    # Arabic output request patterns in French\n    french_to_arabic_patterns = [\n        'en arabe',\n        'r√©ponds en arabe',\n        'r√©pondez en arabe',\n        'traduis en arabe',\n        'traduisez en arabe',\n        'answer in arabic',\n        'respond in arabic',\n        'in arabic',\n        'avec r√©ponse en arabe',\n        'r√©ponse en arabe',\n        'explique en arabe',\n        'expliquez en arabe'\n    ]\n    \n    # French output request patterns in Arabic\n    arabic_to_french_patterns = [\n        'ÿ®ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿäÿ©',\n        'ÿ®ÿßŸÑŸÅÿ±ŸÜÿ≥ÿßŸàŸäÿ©',\n        'ÿßÿ¨ÿ® ÿ®ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿäÿ©',\n        'ÿ£ÿ¨ÿ® ÿ®ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿäÿ©',\n        'ÿßŸÑÿ¨Ÿàÿßÿ® ÿ®ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿäÿ©',\n        'en fran√ßais',\n        'in french',\n        'r√©ponds en fran√ßais'\n    ]\n    \n    # Check for Arabic output request\n    for pattern in french_to_arabic_patterns:\n        if pattern in text_lower:\n            return 'ar'\n    \n    # Check for French output request\n    for pattern in arabic_to_french_patterns:\n        if pattern in text_lower:\n            return 'fr'\n    \n    return None\n\ndef clean_question_for_retrieval(text: str, detected_intent: Optional[str]) -> str:\n    \"\"\"\n    Remove language instruction phrases from question before retrieval\n    \n    Args:\n        text: Original question\n        detected_intent: Detected output language intent\n        \n    Returns:\n        Cleaned question without language instructions\n    \"\"\"\n    if detected_intent is None:\n        return text\n    \n    # Patterns to remove\n    removal_patterns = [\n        # French patterns\n        r'\\s*en arabe\\s*',\n        r'\\s*r√©ponds en arabe\\s*',\n        r'\\s*r√©pondez en arabe\\s*',\n        r'\\s*traduis en arabe\\s*',\n        r'\\s*traduisez en arabe\\s*',\n        r'\\s*answer in arabic\\s*',\n        r'\\s*respond in arabic\\s*',\n        r'\\s*in arabic\\s*',\n        r'\\s*avec r√©ponse en arabe\\s*',\n        r'\\s*r√©ponse en arabe\\s*',\n        r'\\s*explique en arabe\\s*',\n        r'\\s*expliquez en arabe\\s*',\n        # Arabic patterns\n        r'\\s*ÿ®ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿäÿ©\\s*',\n        r'\\s*ÿ®ÿßŸÑŸÅÿ±ŸÜÿ≥ÿßŸàŸäÿ©\\s*',\n        r'\\s*ÿßÿ¨ÿ® ÿ®ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿäÿ©\\s*',\n        r'\\s*ÿ£ÿ¨ÿ® ÿ®ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿäÿ©\\s*',\n        r'\\s*ÿßŸÑÿ¨Ÿàÿßÿ® ÿ®ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿäÿ©\\s*',\n        r'\\s*en fran√ßais\\s*',\n        r'\\s*in french\\s*',\n        r'\\s*r√©ponds en fran√ßais\\s*'\n    ]\n    \n    import re\n    cleaned = text\n    for pattern in removal_patterns:\n        cleaned = re.sub(pattern, ' ', cleaned, flags=re.IGNORECASE)\n    \n    # Clean up extra spaces\n    cleaned = ' '.join(cleaned.split())\n    \n    return cleaned\n\ndef translate_text(self, text: str, source_lang: str, target_lang: str) -> str:\n        \"\"\"Translate text using NLLB-200 model\"\"\"\n        if source_lang == target_lang:\n            return text\n        \n        # Set source and target languages\n        src_code = self.lang_codes.get(source_lang, 'fra_Latn')\n        tgt_code = self.lang_codes.get(target_lang, 'arb_Arab')\n        \n        # Update tokenizer language settings\n        self.translator_tokenizer.src_lang = src_code\n        \n        # Tokenize\n        inputs = self.translator_tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=512\n        ).to(self.translator_model.device)\n        \n        # Generate translation with forced target language\n        translated = self.translator_model.generate(\n            **inputs,\n            forced_bos_token_id=self.translator_tokenizer.convert_tokens_to_ids(tgt_code),\n            max_length=512,\n            num_beams=5,\n            early_stopping=True\n        )\n        \n        # Decode and normalize\n        result = self.translator_tokenizer.decode(translated[0], skip_special_tokens=True)\n        return normalize_arabic_text(result)  # ‚Üê ADD THIS LINE (normalize result)\n\ndef translate_text_standalone(text: str, source_lang: str, target_lang: str) -> str:\n    \"\"\"\n    Standalone translation function using the global translator\n    \"\"\"\n    if source_lang == target_lang:\n        return text\n    \n    # Language codes for NLLB-200\n    lang_codes = {\n        'ar': 'arb_Arab',\n        'fr': 'fra_Latn'\n    }\n    \n    src_code = lang_codes.get(source_lang, 'fra_Latn')\n    tgt_code = lang_codes.get(target_lang, 'arb_Arab')\n    \n    # Update tokenizer language settings\n    translator_tokenizer.src_lang = src_code\n    \n    # Tokenize\n    inputs = translator_tokenizer(\n        text,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=512\n    )\n    \n    # Move to GPU if available\n    if torch.cuda.is_available():\n        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n    \n    # Generate translation with forced target language\n    with torch.no_grad():\n        translated = translator_model.generate(\n            **inputs,\n            forced_bos_token_id=translator_tokenizer.convert_tokens_to_ids(tgt_code),\n            max_length=512,\n            num_beams=5,\n            early_stopping=True\n        )\n    \n    # Decode and normalize\n    result = translator_tokenizer.decode(translated[0], skip_special_tokens=True)\n    return normalize_arabic_text(result)\n\ndef translate_ar_to_fr(text: str) -> str:\n    \"\"\"Translate Arabic to French\"\"\"\n    return translate_text_standalone(text, 'ar', 'fr')\n\ndef translate_fr_to_ar(text: str) -> str:\n    \"\"\"Translate French to Arabic\"\"\"\n    return translate_text_standalone(text, 'fr', 'ar')\n\nprint(\"‚úÖ Translation functions defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T20:13:15.998534Z","iopub.execute_input":"2025-12-25T20:13:15.998847Z","iopub.status.idle":"2025-12-25T20:13:16.016461Z","shell.execute_reply.started":"2025-12-25T20:13:15.998822Z","shell.execute_reply":"2025-12-25T20:13:16.015819Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Translation functions defined!\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"print(\"\\nLoading embedding model...\")\nembedding_model = SentenceTransformer(EMBEDDING_MODEL)\nembedding_dimension = embedding_model.get_sentence_embedding_dimension()\n\nprint(f\"‚úÖ Embedding model loaded!\")\nprint(f\"  Model: {EMBEDDING_MODEL}\")\nprint(f\"  Dimension: {embedding_dimension}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T20:07:17.008104Z","iopub.execute_input":"2025-12-25T20:07:17.008727Z","iopub.status.idle":"2025-12-25T20:07:23.933119Z","shell.execute_reply.started":"2025-12-25T20:07:17.008702Z","shell.execute_reply":"2025-12-25T20:07:23.932421Z"}},"outputs":[{"name":"stderr","text":"WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name dangvantuan/sentence-camembert-large. Creating a new one with mean pooling.\n","output_type":"stream"},{"name":"stdout","text":"\nLoading embedding model...\n‚úÖ Embedding model loaded!\n  Model: dangvantuan/sentence-camembert-large\n  Dimension: 1024\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"Loading LLM for generation...\")\n\ntokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n\nif USE_4BIT_QUANTIZATION and torch.cuda.is_available():\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\"\n    )\n    llm_model = AutoModelForCausalLM.from_pretrained(\n        LLM_MODEL,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n    print(f\"‚úÖ LLM loaded with 4-bit quantization!\")\nelse:\n    llm_model = AutoModelForCausalLM.from_pretrained(\n        LLM_MODEL,\n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n        device_map=\"auto\" if torch.cuda.is_available() else None,\n        trust_remote_code=True\n    )\n    print(f\"‚úÖ LLM loaded!\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"  Device: {device}\")\nprint(f\"  Model: {LLM_MODEL}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T20:01:39.592142Z","iopub.execute_input":"2025-12-25T20:01:39.592459Z","iopub.status.idle":"2025-12-25T20:02:49.488043Z","shell.execute_reply.started":"2025-12-25T20:01:39.592439Z","shell.execute_reply":"2025-12-25T20:02:49.486966Z"}},"outputs":[{"name":"stdout","text":"Loading LLM for generation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"010f6239fdc74785977a00c81e629efa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"741671bf426e414fab36c6aefb28b4e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76aed3e0acf74d4c85fcd836fa87ac9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/670 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfda25045d994671b52ea7ae772750cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afb2bac3471748348e1b822b55e0f41a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bd24b4c3de7408c85c2a97d35e76c3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00007.bin:   0%|          | 0.00/1.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e53bc2bf9dd44d3b44ce29464951015"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00004-of-00007.bin:   0%|          | 0.00/1.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d153b79b5df43e78384873c238904d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00005-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"414ade9891c94f1888bb9dcfa4aaa21c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00007-of-00007.bin:   0%|          | 0.00/1.66G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbb328a1621d49428315a8dcc05dbac3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"147bccb3d9664421a86926643fc34579"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00003-of-00007.bin:   0%|          | 0.00/1.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b026bbcb1b2a48c2b5891e2a7fb96600"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00007.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03422fc92151499fafd5f154c29502cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab7630a71c3741baaeb12495ffb20424"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3bfc810d8d94f469a0a39882db39fda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7378acdd211a49389842680471125bd4"}},"metadata":{}},{"name":"stdout","text":"‚úÖ LLM loaded with 4-bit quantization!\n  Device: cuda\n  Model: bofenghuang/vigogne-2-7b-chat\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(\"\\nLoading existing index and documents...\")\n\n# Load FAISS index\nindex = faiss.read_index(INDEX_PATH)\nprint(f\"‚úÖ FAISS index loaded: {index.ntotal} vectors\")\n\n# Load documents\nwith open(DOCS_PATH, 'r', encoding='utf-8') as f:\n    documents = json.load(f)\nprint(f\"‚úÖ Documents loaded: {len(documents)} documents\")\n\n# Load metadata\nwith open(META_PATH, 'r', encoding='utf-8') as f:\n    metadata = json.load(f)\nprint(f\"‚úÖ Metadata loaded\")\nprint(f\"  Document types: {', '.join(metadata['document_types'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T20:02:49.489722Z","iopub.execute_input":"2025-12-25T20:02:49.490544Z","iopub.status.idle":"2025-12-25T20:02:51.414122Z","shell.execute_reply.started":"2025-12-25T20:02:49.490516Z","shell.execute_reply":"2025-12-25T20:02:51.413381Z"}},"outputs":[{"name":"stdout","text":"\nLoading existing index and documents...\n‚úÖ FAISS index loaded: 18722 vectors\n‚úÖ Documents loaded: 18722 documents\n‚úÖ Metadata loaded\n  Document types: jo_2020, code_civil, jo_2023, jo_2021, jo_2022, jo_2024, jo_2025, code_penale, constitution, code_commerce, code_famille\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def retrieve_documents(query: str, top_k: int = 3) -> List[Tuple[Dict, float]]:\n    \"\"\"\n    Retrieve most relevant documents for a query\n    Query is always in French (translated if needed)\n    \"\"\"\n    query_embedding = embedding_model.encode(\n        [query],\n        convert_to_numpy=True,\n        normalize_embeddings=True\n    )\n    \n    scores, indices = index.search(query_embedding.astype('float32'), top_k)\n    \n    results = []\n    for idx, score in zip(indices[0], scores[0]):\n        if idx < len(documents):\n            results.append((documents[idx], float(score)))\n    \n    return results\n\ndef generate_answer(query: str, context_docs: List[Tuple[Dict, float]]) -> str:\n    \"\"\"\n    Generate answer using retrieved context\n    Query and context are in French\n    \"\"\"\n    # Build context\n    context_parts = []\n    for doc, score in context_docs:\n        doc_type = doc.get('source_document_type', '').upper()\n        header = doc.get('header', '')\n        content = doc.get('content', '')\n        \n        ctx = f\"[{doc_type}] {header}\\n{content}\"\n        context_parts.append(ctx)\n    \n    context = \"\\n\\n\".join(context_parts)\n    \n    # Create prompt\n    prompt = f\"\"\"<|system|>: Tu es un assistant juridique expert en droit alg√©rien. R√©ponds de mani√®re pr√©cise, professionnelle et factuelle en te basant strictement sur le contexte fourni. Si l'information n'est pas dans le contexte, indique-le clairement.\n<|user|>: Contexte juridique:\n{context}\n\nQuestion: {query}\n<|assistant|>:\"\"\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    if torch.cuda.is_available():\n        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n    \n    # Generate\n    with torch.no_grad():\n        outputs = llm_model.generate(\n            **inputs,\n            max_new_tokens=512,\n            temperature=0.7,\n            top_p=0.95,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n            repetition_penalty=1.1\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract answer\n    if \"<|assistant|>:\" in response:\n        answer = response.split(\"<|assistant|>:\")[-1].strip()\n    else:\n        answer = response[len(prompt):].strip()\n    \n    return normalize_arabic_text(answer)\n\ndef query_bilingual_rag(\n    question: str,\n    top_k: int = 3,\n    show_sources: bool = True,\n    force_output_lang: Optional[str] = None\n) -> Dict:\n    \"\"\"\n    Complete bilingual RAG query with smart language intent detection\n    \n    Args:\n        question: User's question in Arabic or French (can include language instructions)\n        top_k: Number of documents to retrieve\n        show_sources: Whether to display sources\n        force_output_lang: Force output language ('ar' or 'fr'), None for auto-detect\n        \n    Returns:\n        Dictionary with answer, language info, and sources\n    \"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"Question: {question}\")\n    \n    # Detect if user explicitly requested specific output language\n    output_lang_intent = detect_output_language_intent(question)\n    \n    # Detect input language\n    input_lang = detect_language(question)\n    print(f\"Detected input language: {'Arabic' if input_lang == 'ar' else 'French' if input_lang == 'fr' else 'Unknown'}\")\n    \n    if output_lang_intent:\n        print(f\"‚ö†Ô∏è  User requested answer in: {'Arabic' if output_lang_intent == 'ar' else 'French'}\")\n    \n    # Determine output language (priority: force > intent > input language)\n    if force_output_lang:\n        output_lang = force_output_lang\n    elif output_lang_intent:\n        output_lang = output_lang_intent\n    else:\n        output_lang = input_lang\n    \n    print(f\"Output language: {'Arabic' if output_lang == 'ar' else 'French'}\")\n    print('='*70)\n    \n    # Clean question by removing language instruction phrases\n    cleaned_question = clean_question_for_retrieval(question, output_lang_intent)\n    if cleaned_question != question:\n        print(f\"\\nüßπ Cleaned question: {cleaned_question}\")\n    \n    # Translate query to French if needed (use cleaned version)\n    french_query = cleaned_question\n    if input_lang == 'ar':\n        print(\"\\nüîÑ Translating query to French...\")\n        french_query = translate_ar_to_fr(cleaned_question)\n        print(f\"  Translated query: {french_query}\")\n    \n    # Retrieve documents\n    print(\"\\nüîç Retrieving relevant documents...\")\n    retrieved_docs = retrieve_documents(french_query, top_k)\n    \n    # Generate answer in French\n    print(\"ü§ñ Generating answer...\")\n    french_answer = generate_answer(french_query, retrieved_docs)\n    \n    # Translate answer to Arabic if needed\n    final_answer = french_answer\n    if output_lang == 'ar':\n        print(\"\\nüîÑ Translating answer to Arabic...\")\n        final_answer = translate_fr_to_ar(french_answer)\n    \n    # Display results\n    print(f\"\\nüìù Answer:\")\n    print_arabic(final_answer)  # ‚Üê CHANGED: Use print_arabic() for proper display\n    print()\n    \n    if show_sources:\n        print(\"üìö Sources:\")\n        for i, (doc, score) in enumerate(retrieved_docs, 1):\n            print(f\"\\n  [{i}] {doc.get('source_document_type', '').upper()}\")\n            print(f\"      {doc.get('header', '')}\")\n            print(f\"      Relevance: {score:.4f}\")\n            content = doc.get('content', '')\n            preview = content[:200] + \"...\" if len(content) > 200 else content\n            print(f\"      {preview}\")\n    \n    return {\n        \"question\": question,\n        \"cleaned_question\": cleaned_question,\n        \"input_language\": input_lang,\n        \"output_language\": output_lang,\n        \"language_intent_detected\": output_lang_intent is not None,\n        \"french_query\": french_query if input_lang == 'ar' else None,\n        \"french_answer\": french_answer if output_lang == 'ar' else None,\n        \"answer\": final_answer,\n        \"sources\": [(doc, score) for doc, score in retrieved_docs]\n    }\n\nprint(\"‚úÖ Bilingual RAG functions defined!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T20:12:17.102528Z","iopub.execute_input":"2025-12-25T20:12:17.102821Z","iopub.status.idle":"2025-12-25T20:12:17.119339Z","shell.execute_reply.started":"2025-12-25T20:12:17.102797Z","shell.execute_reply":"2025-12-25T20:12:17.118581Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Bilingual RAG functions defined!\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*70)\nprint(\"TESTING FRENCH QUERIES\")\nprint(\"=\"*70)\n\nresult_fr = query_bilingual_rag(\n    \"Qu'est-ce que l'Alg√©rie selon la constitution?\",\n    top_k=3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T20:07:35.644200Z","iopub.execute_input":"2025-12-25T20:07:35.644514Z","iopub.status.idle":"2025-12-25T20:07:46.160688Z","shell.execute_reply.started":"2025-12-25T20:07:35.644491Z","shell.execute_reply":"2025-12-25T20:07:46.159987Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nTESTING FRENCH QUERIES\n======================================================================\n\n======================================================================\nQuestion: Qu'est-ce que l'Alg√©rie selon la constitution?\nDetected input language: French\nOutput language: French\n======================================================================\n\nüîç Retrieving relevant documents...\nü§ñ Generating answer...\n\nüìù Answer:\nSelon la Constitution de l'Alg√©rie, l'Alg√©rie est une R√©publique d√©mocratique et populaire. Elle est consid√©r√©e comme une entit√© unique et indivisible. En outre, Alger est d√©sign√© comme la capitale de la R√©publique alg√©rienne. \n\nLe Code civil mentionne √©galement l'application du droit en Alg√©rie, comprenant notamment les livres des fiches d'√©tat civil et l'expression \"en Alg√©rie\", qui englobe tout le territoire alg√©rien, les eaux territoriales, les navires et avions alg√©riens.\n\nüìö Sources:\n\n  [1] CONSTITUTION\n      Title I - Chapter PREMIER - Article 1er\n      Relevance: 0.4948\n      L'Alg√©rie est une R√©publique D√©mocratique et Populaire. Elle est une et indivisible.\n\n  [2] CONSTITUTION\n      Title I - Chapter PREMIER - Article 5\n      Relevance: 0.4773\n      La capitale de la R√©publique est Alger.\n\n  [3] CODE_CIVIL\n      EUXIEME DE LA REPRESENTATION LEGALE - LE LIVRET DE FAMILLE ET LES FICHES D'ETAT CIVIL - Article 5\n      Relevance: 0.4696\n      Article 5: (Modifi√©) - L‚Äôexpression ¬´en Alg√©rie¬ª s‚Äôentend de tout le territoire alg√©rien, des eaux territoriales alg√©riennes, des navires et a√©ronefs alg√©riens. (3) ________________ 41 CHAPITRE II De ...\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*70)\nprint(\"TESTING ARABIC QUERIES\")\nprint(\"=\"*70)\n\nresult_ar = query_bilingual_rag(\n    \"ŸÖÿß ŸáŸä ÿßŸÑÿ¨ÿ≤ÿßÿ¶ÿ± ÿ≠ÿ≥ÿ® ÿßŸÑÿØÿ≥ÿ™Ÿàÿ±ÿü\",\n    top_k=3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T20:13:23.007787Z","iopub.execute_input":"2025-12-25T20:13:23.008439Z","iopub.status.idle":"2025-12-25T20:13:23.017610Z","shell.execute_reply.started":"2025-12-25T20:13:23.008395Z","shell.execute_reply":"2025-12-25T20:13:23.016658Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nTESTING ARABIC QUERIES\n======================================================================\n\n======================================================================\nQuestion: ŸÖÿß ŸáŸä ÿßŸÑÿ¨ÿ≤ÿßÿ¶ÿ± ÿ≠ÿ≥ÿ® ÿßŸÑÿØÿ≥ÿ™Ÿàÿ±ÿü\nDetected input language: Arabic\nOutput language: Arabic\n======================================================================\n\nüîÑ Translating query to French...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1842222482.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m result_ar = query_bilingual_rag(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"ŸÖÿß ŸáŸä ÿßŸÑÿ¨ÿ≤ÿßÿ¶ÿ± ÿ≠ÿ≥ÿ® ÿßŸÑÿØÿ≥ÿ™Ÿàÿ±ÿü\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/1385312022.py\u001b[0m in \u001b[0;36mquery_bilingual_rag\u001b[0;34m(question, top_k, show_sources, force_output_lang)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_lang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ar'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüîÑ Translating query to French...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mfrench_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_ar_to_fr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Translated query: {french_query}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3770467582.py\u001b[0m in \u001b[0;36mtranslate_ar_to_fr\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_ar_to_fr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"Translate Arabic to French\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtranslate_text_standalone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_fr_to_ar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3770467582.py\u001b[0m in \u001b[0;36mtranslate_text_standalone\u001b[0;34m(text, source_lang, target_lang)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# Update tokenizer language settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mtranslator_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# Tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'translator_tokenizer' is not defined"],"ename":"NameError","evalue":"name 'translator_tokenizer' is not defined","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*70)\nprint(\"TESTING LANGUAGE INTENT DETECTION\")\nprint(\"=\"*70)\n\n# Test 1: French query with \"en arabe\" instruction\nprint(\"\\n--- Test 1: French query requesting Arabic answer ---\")\nresult1 = query_bilingual_rag(\n    \"Qu'est-ce que l'Alg√©rie selon la constitution? R√©ponds en arabe\",\n    top_k=3\n)\n\nprint(\"\\n\" + \"=\"*70)\n\n# Test 2: French query with \"answer in arabic\"\nprint(\"\\n--- Test 2: French query with English instruction ---\")\nresult2 = query_bilingual_rag(\n    \"Quels sont les droits fondamentaux? Answer in Arabic\",\n    top_k=3\n)\n\nprint(\"\\n\" + \"=\"*70)\n\n# Test 3: Arabic query with French output request\nprint(\"\\n--- Test 3: Arabic query requesting French answer ---\")\nresult3 = query_bilingual_rag(\n    \"ŸÖÿß ŸáŸä ÿßŸÑÿ≠ÿ±Ÿäÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©ÿü ÿßŸÑÿ¨Ÿàÿßÿ® ÿ®ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿäÿ©\",\n    top_k=3\n)\n\nprint(\"\\n\" + \"=\"*70)\n\n# Test 4: More natural phrasing\nprint(\"\\n--- Test 4: Natural language instruction ---\")\nresult4 = query_bilingual_rag(\n    \"Explique-moi le syst√®me l√©gislatif alg√©rien, avec r√©ponse en arabe s'il te pla√Æt\",\n    top_k=3\n)\n\nprint(\"\\n\" + \"=\"*70)\n\n# Test 5: No language instruction (should use input language)\nprint(\"\\n--- Test 5: No language instruction ---\")\nresult5 = query_bilingual_rag(\n    \"Quelle est la capitale de l'Alg√©rie?\",\n    top_k=3\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T19:39:19.547174Z","iopub.status.idle":"2025-12-25T19:39:19.547491Z","shell.execute_reply.started":"2025-12-25T19:39:19.547358Z","shell.execute_reply":"2025-12-25T19:39:19.547375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}